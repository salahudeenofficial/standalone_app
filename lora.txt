LORA APPLICATION MONITORING METRICS - COMPREHENSIVE REFERENCE
================================================================

This document contains all the metrics currently being tracked in workflow_api.py 
for comprehensive LoRA application monitoring. Use this as a reference to replicate 
the monitoring system in pipeline.py.

================================================================
1. BASELINE CAPTURE METRICS (Before LoRA Application)
================================================================

UNET MODEL BASELINE:
- model_id: Unique object identifier (id(unet_model))
- class: Model class name (type(unet_model).__name__)
- device: Current device placement (unet_model.device)
- patches_count: Number of existing patches (len(unet_model.patches))
- patches_uuid: Unique identifier for patches (unet_model.patches_uuid)
- memory_allocated: GPU memory allocated at baseline (torch.cuda.memory_allocated())
- memory_reserved: GPU memory reserved at baseline (torch.cuda.memory_reserved())

CLIP MODEL BASELINE:
- model_id: Unique object identifier (id(clip_model))
- class: Model class name (type(clip_model).__name__)
- device: Current device placement (clip_model.device)
- patcher_patches_count: Number of patches in CLIP patcher (len(clip_model.patcher.patches))
- patcher_patches_uuid: UUID of CLIP patcher patches (clip_model.patcher.patches_uuid)
- memory_allocated: GPU memory allocated at baseline (torch.cuda.memory_allocated())
- memory_reserved: GPU memory reserved at baseline (torch.cuda.memory_reserved())

LORA FILE STATUS:
- filename: LoRA file name (e.g., "lora.safetensors")
- file_exists: Boolean indicating file presence (os.path.exists())
- file_size: File size in MB (os.path.getsize() / (1024**2))

TIMESTAMP:
- timestamp: When baseline was captured (time.time())

================================================================
2. MODEL IDENTITY CHANGES TRACKING
================================================================

MODEL INSTANCE CHANGES:
- model_cloned: Boolean indicating if model was cloned (original_model is not modified_model)
- class_changed: Boolean indicating if class type changed (type(original_model) != type(modified_model))
- patches_added: Count of new patches added (modified_patch_count - original_patch_count)
- uuid_changed: Boolean indicating if patch UUID changed (original_uuid != modified_uuid)
- original_patch_count: Number of patches before LoRA application
- modified_patch_count: Number of patches after LoRA application

================================================================
3. WEIGHT MODIFICATIONS TRACKING
================================================================

STATE DICTIONARY CHANGES:
- keys_added: List of new keys in modified model (modified_keys - original_keys)
- keys_removed: List of keys removed from original model (original_keys - modified_keys)
- keys_modified: List of keys that exist in both models (original_keys & modified_keys)
- total_keys_original: Count of keys in original model (len(original_keys))
- total_keys_modified: Count of keys in modified model (len(modified_keys))

INDIVIDUAL WEIGHT CHANGES (per key):
- shape_changed: Boolean indicating if weight shape changed (orig_weight.shape != mod_weight.shape)
- dtype_changed: Boolean indicating if weight data type changed (orig_weight.dtype != mod_weight.dtype)
- device_changed: Boolean indicating if weight device changed (orig_weight.device != mod_weight.device)
- original_shape: String representation of original weight shape (str(orig_weight.shape))
- modified_shape: String representation of modified weight shape (str(mod_weight.shape))

================================================================
4. LoRA PATCHES ANALYSIS
================================================================

PATCH DETAILS:
- total_patched_keys: Count of keys that have LoRA patches (len(lora_patches))
- patch_details: Dictionary containing detailed patch information for each key

PER-KEY PATCH INFORMATION:
- strength_patch: LoRA strength parameter applied (patch[0])
- patch_type: Type of LoRA patch ('lora_up', 'lora_down', 'diff', 'unknown')
- patch_data_shape: Type information of patch data (str(type(patch_data)))
- patch_count: Number of patches for each key (len(patch_list))

PATCH TYPE DETECTION:
- lora_up: Detected when 'lora_up.weight' in str(patch_data)
- lora_down: Detected when 'lora_down.weight' in str(patch_data)
- diff: Detected when 'diff' in str(patch_data)
- unknown: When patch type cannot be determined

================================================================
5. MODEL PLACEMENT CHANGES TRACKING
================================================================

DEVICE CHANGES:
- model_device_changed: Boolean indicating if model device changed (original_device != modified_device)
- load_device_changed: Boolean indicating if load device changed (original_load_device != modified_load_device)
- offload_device_changed: Boolean indicating if offload device changed (original_offload_device != modified_offload_device)
- original_device: String representation of original device (str(original_device))
- modified_device: String representation of modified device (str(modified_device))

CLIP-SPECIFIC DEVICE TRACKING:
- clip_model_device: Device of CLIP model itself (clip_model.patcher.model.device)
- clip_patcher_load_device: Load device of CLIP patcher (clip_model.patcher.load_device)
- clip_patcher_offload_device: Offload device of CLIP patcher (clip_model.patcher.offload_device)

================================================================
6. MEMORY IMPACT TRACKING
================================================================

MEMORY CHANGES:
- allocated_change_mb: Change in allocated GPU memory in MB ((current_allocated - baseline_allocated) / (1024**2))
- reserved_change_mb: Change in reserved GPU memory in MB ((current_reserved - baseline_reserved) / (1024**2))
- current_allocated_mb: Current allocated GPU memory in MB (current_allocated / (1024**2))
- current_reserved_mb: Current reserved GPU memory in MB (current_reserved / (1024**2))

MEMORY CALCULATION:
- Uses torch.cuda.memory_allocated() and torch.cuda.memory_reserved()
- Converts from bytes to MB for readability
- Calculates difference between baseline and current memory usage

================================================================
7. COMPREHENSIVE ANALYSIS RESULTS
================================================================

LORA APPLICATION SUCCESS:
- lora_application_success: Boolean indicating if LoRA was applied successfully (lora_result is not None)
- models_returned: Count of models returned from LoRA application (len(lora_result))

ERROR HANDLING:
- error: Error message if memory calculation fails
- exception_details: Full exception information for debugging (from try-catch blocks)

================================================================
8. MONITORING METHODS AND FUNCTIONS
================================================================

CORE MONITORING METHODS:
1. capture_lora_baseline(unet_model, clip_model)
   - Captures complete baseline state before LoRA application
   - Returns comprehensive baseline dictionary

2. track_model_identity_changes(original_model, modified_model, model_type)
   - Tracks changes in model identity and structure
   - Returns identity change analysis dictionary

3. track_weight_modifications(original_model, modified_model, model_type)
   - Tracks how LoRA modifies model weights
   - Returns weight modification analysis dictionary

4. analyze_lora_patches(modified_model, model_type)
   - Analyzes specific LoRA patches applied to the model
   - Returns patch analysis dictionary

5. track_model_placement_changes(original_model, modified_model, model_type)
   - Tracks changes in model device placement
   - Returns placement change analysis dictionary

6. calculate_memory_change(baseline_info, current_model)
   - Calculates memory usage changes for a model
   - Returns memory change analysis dictionary

7. analyze_lora_application_results(baseline, modified_unet, modified_clip, lora_result)
   - Comprehensive analysis of LoRA application results
   - Returns complete analysis dictionary

8. print_lora_analysis_summary(analysis)
   - Prints comprehensive LoRA application analysis
   - Formats output for human readability

================================================================
9. DATA STRUCTURE FORMATS
================================================================

BASELINE STRUCTURE:
{
    'timestamp': float,
    'unet': {
        'model_id': int,
        'class': str,
        'device': torch.device,
        'patches_count': int,
        'patches_uuid': uuid.UUID,
        'memory_allocated': int,
        'memory_reserved': int
    },
    'clip': {
        'model_id': int,
        'class': str,
        'device': torch.device,
        'patcher_patches_count': int,
        'patcher_patches_uuid': uuid.UUID,
        'memory_allocated': int,
        'memory_reserved': int
    }
}

ANALYSIS STRUCTURE:
{
    'lora_application_success': bool,
    'models_returned': int,
    'unet_changes': dict,  # Identity changes
    'clip_changes': dict,   # Identity changes
    'unet_weight_changes': dict,  # Weight modifications
    'clip_weight_changes': dict,  # Weight modifications
    'unet_lora_patches': dict,    # Patch analysis
    'clip_lora_patches': dict,    # Patch analysis
    'placement_changes': {
        'unet': dict,  # Placement changes
        'clip': dict   # Placement changes
    },
    'memory_impact': {
        'unet_memory_change': dict,  # Memory changes
        'clip_memory_change': dict   # Memory changes
    }
}

================================================================
10. INTEGRATION POINTS
================================================================

MONITORING INTEGRATION:
- Before LoRA application: Call capture_lora_baseline()
- During LoRA application: Monitor for exceptions and errors
- After LoRA application: Call analyze_lora_application_results()
- Final summary: Call print_lora_analysis_summary()

ERROR HANDLING INTEGRATION:
- Wrap LoRA application in try-catch blocks
- Capture and report specific error types
- Provide fallback behavior for failed LoRA application

MEMORY MONITORING INTEGRATION:
- Capture baseline memory before LoRA application
- Monitor memory during LoRA application
- Calculate memory impact after LoRA application
- Report memory usage changes

================================================================
11. OUTPUT FORMATTING
================================================================

CONSOLE OUTPUT FORMAT:
- Uses emojis for visual clarity (🔍, ✅, ❌, ⚠️, 🔧, 💾, 📊)
- Structured sections with clear headers
- Consistent formatting with separators (===)
- Human-readable metrics with units (MB, seconds, etc.)

SUMMARY OUTPUT:
- Step-by-step breakdown of all monitored parameters
- Pass/Fail indicators for critical checks
- Memory usage summaries with before/after comparison
- Error reporting with specific failure details

================================================================
12. IMPLEMENTATION NOTES
================================================================

PYTORCH INTEGRATION:
- Requires torch.cuda.is_available() checks
- Handles cases where CUDA is not available
- Graceful fallback for non-GPU environments

MODEL ACCESS PATTERNS:
- Uses getattr() for safe attribute access
- Handles missing attributes gracefully
- Provides fallback values for inaccessible properties

EXCEPTION HANDLING:
- Comprehensive try-catch blocks around model operations
- Detailed error reporting for debugging
- Continues execution even if some monitoring fails

================================================================
END OF METRICS REFERENCE
================================================================

Total Parameters Tracked: 50+ individual metrics
Monitoring Coverage: Complete LoRA application lifecycle
Integration Level: Deep integration with ComfyUI model system
Error Handling: Comprehensive exception handling and reporting
Output Format: Human-readable console output with structured data 