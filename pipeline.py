#!/usr/bin/env python3
"""
Standalone Reference Image + Control Video to Output Video Pipeline
Based on ComfyUI components but stripped of WebSocket, graph execution, and UI dependencies

This pipeline now properly leverages ComfyUI's native memory management system:
- UNET models: Automatically managed by ModelPatcher (GPU/CPU swapping)
- VAE models: Built-in memory management with automatic loading/unloading
- CLIP models: Automatically managed by ModelPatcher
- All memory management: Handled by ComfyUI's proven system
"""

import torch
import os
import sys
from pathlib import Path

# Add the current directory to Python path
sys.path.insert(0, str(Path(__file__).parent))

# Add ComfyUI path for utilities
sys.path.insert(0, str(Path(__file__).parent / "comfy"))

# Import ComfyUI modules in the same order as the working UNETLoader node
import comfy.diffusers_load
import comfy.samplers
import comfy.sample
import comfy.sd
import comfy.utils
import comfy.controlnet
import comfy.model_management
import comfy.clip_vision

# Import psutil for system information in diagnostic summary
try:
    import psutil
except ImportError:
    print("Warning: psutil not available, system information will be limited")
    psutil = None

from components.lora_loader import LoraLoader
from components.text_encoder import CLIPTextEncode
from components.model_sampling import ModelSamplingSD3
from components.video_generator import WanVaceToVideo
from components.sampler import KSampler
from components.video_processor import TrimVideoLatent
from components.vae_decoder import VAEDecode
from components.video_export import VideoExporter
from components.chunked_processor import ChunkedProcessor

class ReferenceVideoPipeline:
    """
    Standalone Reference Image + Control Video to Output Video Pipeline
    
    Memory Management Philosophy (explicit ModelPatcher control):
    - UNET models: Explicitly managed using ModelPatcher.unpatch_model(device_to=offload_device)
    - VAE models: Moved to GPU for operations, then to CPU for memory management
    - CLIP models: Explicitly managed using clip.patcher.unpatch_model(device_to=offload_device)
    - All memory management: Explicit control using proven ModelPatcher methods
    
    This approach ensures:
    1. Heavy models (UNET) get explicit GPU/CPU swapping via ModelPatcher
    2. VAE models get explicit device placement control
    3. Light models (CLIP) get the same explicit ModelPatcher control
    4. Full control over when models are loaded/unloaded
    5. Uses the exact same working logic as test_comfyui_integration.py
    """
    def __init__(self, models_dir="models"):
        """Initialize the pipeline with model directory"""
        self.models_dir = models_dir
        self.setup_model_paths()
        
        # Initialize chunked processor for optimal frame processing
        self.chunked_processor = ChunkedProcessor()
        
        # Start with conservative chunking for better memory management
        try:
            self.chunked_processor.set_chunking_strategy('conservative')
            print("‚úÖ Chunked processor initialized with conservative strategy")
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not set chunking strategy: {e}")
            print("   Using default chunking strategy")
        
        # Verify chunked processor is working
        try:
            if hasattr(self.chunked_processor, 'current_strategy'):
                print(f"‚úÖ Chunked processor strategy: {self.chunked_processor.current_strategy}")
            if hasattr(self.chunked_processor, 'default_chunk_sizes'):
                print("‚úÖ Chunked processor has default chunk sizes configured")
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Chunked processor verification failed: {e}")
        
        # Initialize OOM debugging checklist
        self.oom_checklist = {
            'baseline_memory': None,
            'model_loading': None,
            'lora_application': None,
            'text_encoding': None,
            'model_sampling': None,
            'gpu_capability_test': None,  # Add GPU capability test results
            'vae_encoding': None,
            'unet_sampling': None,
            'video_trimming': None,
            'vae_decoding': None,
            'video_export': None,
            'final_cleanup': None
        }
        
        # Memory thresholds for each phase
        self.memory_thresholds = {
            'baseline': 100,           # MB - should be very low
            'model_loading': 100,      # MB - ComfyUI lazy loading (models loaded on-demand)
            'lora_application': 200,   # MB - LoRA applied (still lazy loading)
            'text_encoding': 1000,     # MB - models may be loaded to GPU for encoding
            'model_sampling': 2000,    # MB - models loaded for patching
            'gpu_capability_test': 1000, # MB - should be low during testing
            'vae_encoding': 8000,     # MB - VAE encoding in progress (models loaded)
            'unet_sampling': 40000,   # MB - UNET sampling needs ~33GB (realistic)
            'vae_decoding': 8000,     # MB - VAE decoding in progress
            'final_cleanup': 100,      # MB - back to baseline
            'video_trimming': 100,      # MB - back to baseline
            'video_export': 100         # MB - back to baseline
        }
    
    def _check_memory_usage(self, phase_name, expected_threshold=None):
        """Check memory usage and update OOM checklist"""
        if not torch.cuda.is_available():
            return True
            
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        
        # Update checklist
        self.oom_checklist[phase_name] = {
            'allocated_mb': allocated,
            'reserved_mb': reserved,
            'timestamp': phase_name,
            'status': 'PASS' if expected_threshold is None else ('PASS' if allocated <= expected_threshold else 'FAIL')
        }
        
        # Get threshold for this phase
        threshold = expected_threshold or self.memory_thresholds.get(phase_name, 0)
        
        print(f"üîç {phase_name.upper()} MEMORY CHECK:")
        print(f"   Allocated: {allocated:.1f} MB")
        print(f"   Reserved: {reserved:.1f} MB")
        print(f"   Threshold: {threshold:.1f} MB")
        print(f"   Status: {self.oom_checklist[phase_name]['status']}")
        
        if allocated > threshold:
            print(f"   ‚ö†Ô∏è  WARNING: Memory usage ({allocated:.1f} MB) exceeds threshold ({threshold:.1f} MB)")
            print(f"   üí° This phase may be at risk of OOM errors")
        
        return allocated <= threshold
    
    def _print_oom_checklist(self):
        """Print the complete OOM debugging checklist"""
        print("\n" + "="*80)
        print("üö® OOM DEBUGGING CHECKLIST")
        print("="*80)
        
        for phase, data in self.oom_checklist.items():
            if data is None:
                print(f"‚ùå {phase}: NOT EXECUTED")
                continue
                
            status_icon = "‚úÖ" if data['status'] == 'PASS' else "‚ùå" if data['status'] == 'FAIL' else "‚ö†Ô∏è"
            print(f"{status_icon} {phase}:")
            print(f"   Allocated: {data['allocated_mb']:.1f} MB")
            print(f"   Reserved: {data['reserved_mb']:.1f} MB")
            print(f"   Status: {data['status']}")
            
            # Special handling for GPU capability test
            if phase == 'gpu_capability_test' and 'gpu_capable' in data:
                print(f"   GPU Capable: {'‚úÖ YES' if data['gpu_capable'] else '‚ùå NO'}")
                
                # Show test results if available
                if 'test_results' in data and data['test_results']:
                    test_results = data['test_results']
                    print(f"   Test Results:")
                    
                    # Show VRAM info
                    if 'vram_total' in test_results:
                        print(f"     Total VRAM: {test_results['vram_total']:.2f} GB")
                        print(f"     Free VRAM: {test_results['vram_free']:.2f} GB")
                        print(f"     Fragmentation: {test_results.get('fragmentation_ratio', 0):.1%}")
                    
                    # Show test outcomes
                    for test_name, result in test_results.items():
                        if test_name.endswith('_test') and isinstance(result, bool):
                            test_status = "‚úÖ PASS" if result else "‚ùå FAIL"
                            print(f"     {test_name}: {test_status}")
                        elif test_name == 'vae_memory_estimate':
                            print(f"     VAE Memory Estimate: {result:.1f} MB")
                        elif test_name == 'vae_device':
                            print(f"     VAE Device: {result}")
                        elif test_name == 'error':
                            print(f"     Error: {result}")
        
        print("="*80)
        
        # Summary analysis
        failed_phases = [phase for phase, data in self.oom_checklist.items() 
                        if data is not None and data['status'] == 'FAIL']
        
        if failed_phases:
            print(f"üö® PROBLEM PHASES: {', '.join(failed_phases)}")
            print("üí° These phases exceeded memory thresholds and may cause OOM errors")
        else:
            print("‚úÖ ALL PHASES PASSED MEMORY THRESHOLDS")
        
        # GPU capability summary
        gpu_test_data = self.oom_checklist.get('gpu_capability_test')
        if gpu_test_data and 'gpu_capable' in gpu_test_data:
            if gpu_test_data['gpu_capable']:
                print("‚úÖ GPU CAPABILITY: GPU is capable of VAE encoding")
            else:
                print("‚ùå GPU CAPABILITY: GPU is NOT capable of VAE encoding - using CPU fallback")
        
        print("="*80)
    
    def _check_model_placement(self, phase_name, expected_models):
        """Check that models are in the expected devices"""
        print(f"üîç {phase_name.upper()} MODEL PLACEMENT CHECK:")
        
        model_status = {}
        
        # Check UNET placement
        if 'unet' in expected_models:
            try:
                if hasattr(self, 'model') and self.model is not None:
                    if hasattr(self.model, 'model') and hasattr(self.model.model, 'device'):
                        model_status['unet'] = str(self.model.model.device)
                        print(f"   UNET: {model_status['unet']}")
                    elif hasattr(self.model, 'device'):
                        model_status['unet'] = str(self.model.device)
                        print(f"   UNET: {model_status['unet']}")
                    else:
                        model_status['unet'] = 'LOADED (device not accessible)'
                        print(f"   UNET: {model_status['unet']}")
                else:
                    model_status['unet'] = 'NOT LOADED'
                    print(f"   UNET: {model_status['unet']}")
            except Exception as e:
                model_status['unet'] = f'ERROR: {e}'
                print(f"   UNET: {model_status['unet']}")
        
        # Check VAE placement
        if 'vae' in expected_models:
            try:
                if hasattr(self, 'vae') and self.vae is not None:
                    if hasattr(self.vae, 'device'):
                        model_status['vae'] = str(self.vae.device)
                        print(f"   VAE: {model_status['vae']}")
                    elif hasattr(self.vae, 'first_stage_model') and hasattr(self.vae.first_stage_model, 'device'):
                        model_status['vae'] = str(self.vae.first_stage_model.device)
                        print(f"   VAE: {model_status['vae']}")
                    else:
                        model_status['vae'] = 'LOADED (device not accessible)'
                        print(f"   VAE: {model_status['vae']}")
                else:
                    model_status['vae'] = 'NOT LOADED'
                    print(f"   VAE: {model_status['vae']}")
            except Exception as e:
                model_status['vae'] = f'ERROR: {e}'
                print(f"   VAE: {model_status['vae']}")
        
        # Check CLIP placement
        if 'clip' in expected_models:
            try:
                if hasattr(self, 'clip_model') and self.clip_model is not None:
                    if hasattr(self.clip_model, 'patcher') and hasattr(self.clip_model.patcher, 'model'):
                        if hasattr(self.clip_model.patcher.model, 'device'):
                            model_status['clip'] = str(self.clip_model.patcher.model.device)
                            print(f"   CLIP: {model_status['clip']}")
                        else:
                            model_status['clip'] = 'LOADED (device not accessible)'
                            print(f"   CLIP: {model_status['clip']}")
                    elif hasattr(self.clip_model, 'device'):
                        model_status['clip'] = str(self.clip_model.device)
                        print(f"   CLIP: {model_status['clip']}")
                    else:
                        model_status['clip'] = 'LOADED (device not accessible)'
                        print(f"   CLIP: {model_status['clip']}")
                else:
                    model_status['clip'] = 'NOT LOADED'
                    print(f"   CLIP: {model_status['clip']}")
            except Exception as e:
                model_status['clip'] = f'ERROR: {e}'
                print(f"   CLIP: {model_status['clip']}")
        
        return model_status
    
    def _verify_memory_management(self, phase_name, expected_models):
        """Verify that memory management is working correctly after each step"""
        print(f"üîç {phase_name.upper()} MEMORY MANAGEMENT VERIFICATION:")
        
        if not torch.cuda.is_available():
            print("   ‚ö†Ô∏è  CUDA not available, skipping GPU memory verification")
            return True
        
        current_allocated = torch.cuda.memory_allocated() / 1024**2
        current_reserved = torch.cuda.memory_reserved() / 1024**2
        
        print(f"   Current GPU Memory: {current_allocated:.1f} MB allocated, {current_reserved:.1f} MB reserved")
        
        # Check if models are properly offloaded
        models_offloaded = True
        models_checked = 0
        
        for model_name in expected_models:
            if model_name == 'unet' and hasattr(self, 'model') and self.model is not None:
                models_checked += 1
                if hasattr(self.model, 'model') and hasattr(self.model.model, 'device'):
                    device = str(self.model.model.device)
                    if device != 'cpu':
                        print(f"   ‚ùå UNET still on GPU: {device}")
                        models_offloaded = False
                    else:
                        print(f"   ‚úÖ UNET properly offloaded to: {device}")
                elif hasattr(self.model, 'device'):
                    device = str(self.model.device)
                    if device != 'cpu':
                        print(f"   ‚ùå UNET still on GPU: {device}")
                        models_offloaded = False
                    else:
                        print(f"   ‚úÖ UNET properly offloaded to: {device}")
                else:
                    print(f"   ‚ö†Ô∏è  UNET loaded but device not accessible")
            
            elif model_name == 'clip' and hasattr(self, 'clip_model') and self.clip_model is not None:
                models_checked += 1
                if hasattr(self.clip_model, 'patcher') and hasattr(self.clip_model.patcher, 'model'):
                    if hasattr(self.clip_model.patcher.model, 'device'):
                        device = str(self.clip_model.patcher.model.device)
                        if device != 'cpu':
                            print(f"   ‚ùå CLIP still on GPU: {device}")
                            models_offloaded = False
                        else:
                            print(f"   ‚úÖ CLIP properly offloaded to: {device}")
                    else:
                        print(f"   ‚ö†Ô∏è  CLIP loaded but device not accessible")
                elif hasattr(self.clip_model, 'device'):
                    device = str(self.clip_model.device)
                    if device != 'cpu':
                        print(f"   ‚ùå CLIP still on GPU: {device}")
                        models_offloaded = False
                    else:
                        print(f"   ‚úÖ CLIP properly offloaded to: {device}")
                else:
                    print(f"   ‚ö†Ô∏è  CLIP loaded but device not accessible")
            
            elif model_name == 'vae' and hasattr(self, 'vae') and self.vae is not None:
                models_checked += 1
                if hasattr(self.vae, 'device'):
                    device = str(self.vae.device)
                    if device != 'cpu':
                        print(f"   ‚ùå VAE still on GPU: {device}")
                        models_offloaded = False
                    else:
                        print(f"   ‚úÖ VAE properly offloaded to: {device}")
                elif hasattr(self.vae, 'first_stage_model') and hasattr(self.vae.first_stage_model, 'device'):
                    device = str(self.vae.first_stage_model.device)
                    if device != 'cpu':
                        print(f"   ‚ùå VAE still on GPU: {device}")
                        models_offloaded = False
                    else:
                        print(f"   ‚úÖ VAE properly offloaded to: {device}")
                else:
                    print(f"   ‚ö†Ô∏è  VAE loaded but device not accessible")
        
        if models_checked == 0:
            print(f"   ‚ÑπÔ∏è  No models loaded yet for {phase_name}")
            return True
        elif models_offloaded:
            print(f"   ‚úÖ All loaded models properly offloaded to CPU")
        else:
            print(f"   ‚ö†Ô∏è  Some models still on GPU - memory management may be incomplete")
        
        return models_offloaded
    
    def _verify_chunking_strategy(self, phase_name, processing_plan):
        """Verify that chunking strategy is properly configured for the current phase"""
        print(f"üîç {phase_name.upper()} CHUNKING STRATEGY VERIFICATION:")
        
        if not processing_plan:
            print("   ‚ö†Ô∏è  No processing plan available")
            return False
        
        # Check chunking configuration for current phase
        phase_chunks = {}
        
        if 'vae_encode' in processing_plan:
            phase_chunks['vae_encode'] = processing_plan['vae_encode']
        
        if 'unet_process' in processing_plan:
            phase_chunks['unet_process'] = processing_plan['unet_process']
        
        if 'vae_decode' in processing_plan:
            phase_chunks['vae_decode'] = processing_plan['vae_decode']
        
        if not phase_chunks:
            print("   ‚ö†Ô∏è  No chunking configuration found for current phase")
            return False
        
        print("   Chunking Configuration:")
        for operation, config in phase_chunks.items():
            chunk_size = config.get('chunk_size', 'N/A')
            num_chunks = config.get('num_chunks', 'N/A')
            print(f"     {operation}: {chunk_size} items per chunk, {num_chunks} total chunks")
        
        # Verify chunked processor is configured
        if hasattr(self, 'chunked_processor') and self.chunked_processor is not None:
            try:
                strategy = self.chunked_processor.current_strategy
                print(f"   Chunked Processor Strategy: {strategy}")
                
                if strategy == 'conservative':
                    print("   ‚úÖ Using conservative chunking for memory efficiency")
                elif strategy == 'aggressive':
                    print("   ‚ö†Ô∏è  Using aggressive chunking - may use more memory")
                elif strategy == 'ultra_conservative':
                    print("   ‚úÖ Using ultra-conservative chunking for maximum memory efficiency")
                elif strategy == 'balanced':
                    print("   ‚öñÔ∏è  Using balanced chunking strategy")
                else:
                    print(f"   ‚ÑπÔ∏è  Using custom chunking strategy: {strategy}")
                    
                # Additional chunked processor verification
                if hasattr(self.chunked_processor, 'default_chunk_sizes'):
                    print("   ‚úÖ Chunked processor has default chunk sizes configured")
                if hasattr(self.chunked_processor, 'chunking_strategies'):
                    print("   ‚úÖ Chunked processor has chunking strategies configured")
                    
            except AttributeError as e:
                print(f"   ‚ùå Chunked processor missing attribute: {e}")
                return False
            except Exception as e:
                print(f"   ‚ùå Error accessing chunked processor: {e}")
                return False
        else:
            print("   ‚ùå Chunked processor not available")
            return False
        
        return True
        
    def setup_model_paths(self):
        """Setup model paths for the standalone app"""
        # Get the script directory for absolute paths
        script_dir = Path(__file__).parent
        models_dir = script_dir / self.models_dir
        
        # Create model directories if they don't exist
        os.makedirs(models_dir / "diffusion_models", exist_ok=True)  # For UNET models
        os.makedirs(models_dir / "text_encoders", exist_ok=True)     # For CLIP models
        os.makedirs(models_dir / "vaes", exist_ok=True)             # For VAE models
        os.makedirs(models_dir / "loras", exist_ok=True)            # For LoRA models
        
        # Set environment variables for model paths
        os.environ["COMFY_MODEL_PATH"] = str(models_dir)
        
    def run_pipeline(self, 
                    unet_model_path,
                    clip_model_path,
                    vae_model_path,
                    lora_path=None,
                    positive_prompt="",
                    negative_prompt="",
                    control_video_path=None,
                    reference_image_path=None,
                    width=480,
                    height=832,
                    length=37,
                    batch_size=1,
                    strength=1.0,
                    seed=270400132721985,
                    steps=4,
                    cfg=1.0,
                    sampler_name="ddim",
                    scheduler="normal",
                    denoise=1.0,
                    output_path="output.mp4"):
        """
        Run the complete pipeline from reference image + control video to output video
        
        This pipeline now uses explicit ModelPatcher memory management (same as working test):
        - All models are loaded using ComfyUI's native functions
        - ModelPatcher explicitly manages UNET/CLIP memory with device_to=offload_device
        - VAE moved to GPU for operations, then to CPU for memory management
        - Explicit memory management calls using proven working logic
        """
        
        # Establish baseline memory state for OOM debugging
        print("üîç ESTABLISHING BASELINE MEMORY STATE...")
        
        # DEBUG: Check baseline memory state
        if torch.cuda.is_available():
            baseline_allocated = torch.cuda.memory_allocated() / 1024**2
            baseline_reserved = torch.cuda.memory_reserved() / 1024**2
            total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**2
            
            print(f"üîç DEBUG: Baseline memory check:")
            print(f"   CUDA available: {torch.cuda.is_available()}")
            print(f"   Current device: {torch.cuda.current_device()}")
            print(f"   Device properties: {torch.cuda.get_device_properties(0).name}")
            print(f"   Total VRAM: {total_vram:.1f} GB")
            print(f"   Allocated: {baseline_allocated:.1f} MB")
            print(f"   Reserved: {baseline_reserved:.1f} MB")
            
            if baseline_allocated == 0.0 and baseline_reserved == 0.0:
                print("   ‚ö†Ô∏è  WARNING: Baseline memory shows 0.0 MB!")
                print("   üîç This suggests either:")
                print("      - No models loaded yet (expected at start)")
                print("      - Models loaded to CPU instead of GPU")
                print("      - ComfyUI using lazy loading strategy")
                print("      - Memory measurement issue")
            
            # Store baseline for later comparison
            self.baseline_allocated = baseline_allocated
            self.baseline_reserved = baseline_reserved
        else:
            print("üîç CUDA not available, skipping baseline memory check")
            self.baseline_allocated = 0
            self.baseline_reserved = 0
        
        self._check_memory_usage('baseline_memory', expected_threshold=100)
        
        # Generate chunked processing plan
        print("Generating chunked processing plan...")
        
        # Check VRAM status and adjust strategy if needed
        self.chunked_processor.should_adjust_strategy()
        
        # Force conservative chunking if we have limited VRAM
        try:
            vram_status = self.chunked_processor.get_vram_status()
            if vram_status.get('available', False):
                total_vram_gb = vram_status.get('total_gb', 0)
                if total_vram_gb < 12.0:  # Less than 12GB VRAM
                    print(f"Limited VRAM detected ({total_vram_gb:.1f} GB), forcing conservative chunking")
                    self.chunked_processor.force_conservative_chunking()
                elif total_vram_gb < 8.0:  # Less than 8GB VRAM
                    print(f"Very limited VRAM detected ({total_vram_gb:.1f} GB), forcing ultra-conservative chunking")
                    self.chunked_processor.force_ultra_conservative_chunking()
        except Exception as e:
            print(f"Warning: Could not check VRAM status: {e}")
        
        processing_plan = self.chunked_processor.get_processing_plan(
            frame_count=length,
            width=width,
            height=height,
            operations=['vae_encode', 'unet_process', 'vae_decode']
        )
        self.chunked_processor.print_processing_plan(processing_plan)
        
        try:
            # === STEP 1 START: MODEL LOADING ===
            # 1. Load Diffusion Model Components using ComfyUI's native system
            print("1. Loading diffusion model components using ComfyUI...")
            
            # Import ComfyUI's model loading functions
            import comfy.sd
            import comfy.model_management
            
            # Establish baseline memory state
            print("1a. Establishing baseline memory state...")
            if torch.cuda.is_available():
                baseline_allocated = torch.cuda.memory_allocated() / 1024**2
                baseline_reserved = torch.cuda.memory_reserved() / 1024**2
                print(f"Baseline VRAM - Allocated: {baseline_allocated:.1f} MB, Reserved: {baseline_reserved:.1f} MB")
            
            # Load the models using ComfyUI's native functions
            print("1a. Loading individual components...")
            
            # Debug: Show the actual paths being used
            print(f"1a. Current working directory: {os.getcwd()}")
            print(f"1a. UNET path: {unet_model_path}")
            print(f"1a. CLIP path: {clip_model_path}")
            print(f"1a. VAE path: {vae_model_path}")
            print(f"1a. LoRA path: {lora_path}")
            
            # Check if files exist
            print(f"1a. UNET file exists: {os.path.exists(unet_model_path)}")
            print(f"1a. CLIP file exists: {os.path.exists(clip_model_path)}")
            print(f"1a. VAE file exists: {os.path.exists(vae_model_path)}")
            if lora_path:
                print(f"1a. LoRA file exists: {os.path.exists(lora_path)}")
            
            # Use ComfyUI's CheckpointLoader approach - load all models together
            # This ensures proper model type detection and compatibility
            print("1a. Loading models using ComfyUI's CheckpointLoader approach...")
            
            # Use the simple approach - load_diffusion_model should work for WAN models
            print("1a. Using simple load_diffusion_model approach for WAN models...")
            
            # Load UNET - this should work now that we understand the issue
            print("1a. Loading UNET with load_diffusion_model...")
            model = comfy.sd.load_diffusion_model(unet_model_path)
            
            print(f"1a. ‚úÖ UNET loaded: {type(model)}")
            
            # Load CLIP with WAN type
            print("1a. Loading CLIP with WAN type...")
            print(f"1a. DEBUG: CLIP model path: {clip_model_path}")
            print(f"1a. DEBUG: CLIP model path exists: {Path(clip_model_path).exists()}")
            print(f"1a. DEBUG: CLIP type: {comfy.sd.CLIPType.WAN}")
            
            try:
                clip_model = comfy.sd.load_clip(ckpt_paths=[clip_model_path], clip_type=comfy.sd.CLIPType.WAN)
                print(f"1a. ‚úÖ CLIP loaded successfully")
            except Exception as e:
                print(f"1a. ‚ùå ERROR loading CLIP: {e}")
                import traceback
                traceback.print_exc()
                raise
            
            print(f"1a. ‚úÖ CLIP loaded: {type(clip_model)}")
            
            # Debug: Check what type of CLIP model was actually loaded
            print(f"1a. DEBUG: CLIP model class: {clip_model.__class__.__name__}")
            print(f"1a. DEBUG: CLIP model type: {type(clip_model)}")
            
            # Check if it's actually a WAN T5 model
            if hasattr(clip_model, 'patcher') and hasattr(clip_model.patcher, 'model'):
                print(f"1a. DEBUG: CLIP patcher model class: {clip_model.patcher.model.__class__.__name__}")
                if hasattr(clip_model.patcher.model, 'transformer'):
                    print(f"1a. DEBUG: CLIP transformer class: {clip_model.patcher.model.transformer.__class__.__name__}")
                    if hasattr(clip_model.patcher.model.transformer, 'shared'):
                        print(f"1a. DEBUG: CLIP embedding size: {clip_model.patcher.model.transformer.shared.embedding_dim}")
            
            # Check available methods
            print(f"1a. DEBUG: CLIP available methods: {[m for m in dir(clip_model) if not m.startswith('_') and 'encode' in m.lower()]}")
            
            # Load VAE
            print("1a. Loading VAE...")
            vae_sd = comfy.utils.load_torch_file(vae_model_path)
            vae = comfy.sd.VAE(sd=vae_sd)
            vae.throw_exception_if_invalid()
            
            print(f"1a. ‚úÖ VAE loaded: {type(vae)}")
            
            # Note: The model type detection issue is in ComfyUI's model_detection.py
            # The WAN model uses no prefix (keys like 'head.modulation') but the detection
            # logic defaults to 'model.' prefix, causing it to be detected as FLOW instead of WAN
            # This is a ComfyUI bug, not our pipeline issue
            
            # Verify that the models were loaded correctly

            # Check if models need to be explicitly loaded to GPU
         
            # Check memory after letting ComfyUI handle loading
      
            # OOM Checklist: Check memory after model loading

            print("="*80)
            
            print("‚úÖ Step 1 completed: Model Loading")
            # === STEP 1 END: MODEL LOADING ===
            
            # === STEP 2 START: LORA APPLICATION ===
            # 2. Apply LoRA if specified
            if lora_path:
                print("2. Applying LoRA...")
                lora_loader = LoraLoader()
                model, clip_model = lora_loader.load_lora(
                    model, clip_model, lora_path, 0.5, 1.0
                )
                
                # ComfyUI automatically tracks these modified models through ModelPatcher
                print("2a. LoRA applied, models updated")
                print("2a. ModelPatcher automatically preserves LoRA patches")
                
                # OOM Checklist: Check memory after LoRA application
                self._check_memory_usage('lora_application', expected_threshold=200)
                
                # COMPREHENSIVE VERIFICATION AFTER LoRA APPLICATION
                print("\n" + "="*80)
                print("üîç STEP 2 COMPLETE: COMPREHENSIVE VERIFICATION")
                print("="*80)
                
                # 1. Model Placement Verification
                print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
                model_placement = self._check_model_placement('lora_application', ['unet', 'clip'])
                
                # 2. Memory Management Verification
                print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
                memory_management = self._verify_memory_management('lora_application', ['unet', 'clip'])
                
                # 3. Chunking Strategy Verification
                print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
                chunking_strategy = self._verify_chunking_strategy('lora_application', processing_plan)
                
                # 4. Summary
                print("\nüìä STEP 2 SUMMARY:")
                print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
                print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
                print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
                
                if not all([model_placement, memory_management, chunking_strategy]):
                    print("   ‚ö†Ô∏è  Some verifications failed - pipeline may have issues")
                else:
                    print("   ‚úÖ All verifications passed - pipeline ready for next step")
                
                print("="*80)
            else:
                print("2. No LoRA specified, skipping LoRA application")
                print("2a. Models remain in original state")
                
                # COMPREHENSIVE VERIFICATION AFTER LoRA SKIP
                print("\n" + "="*80)
                print("üîç STEP 2 COMPLETE: COMPREHENSIVE VERIFICATION (No LoRA)")
                print("="*80)
                
                # 1. Model Placement Verification
                print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
                model_placement = self._check_model_placement('lora_application', ['unet', 'clip'])
                
                # 2. Memory Management Verification
                print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
                memory_management = self._verify_memory_management('lora_application', ['unet', 'clip'])
                
                # 3. Chunking Strategy Verification
                print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
                chunking_strategy = self._verify_chunking_strategy('lora_application', processing_plan)
                
                # 4. Summary
                print("\nüìä STEP 2 SUMMARY:")
                print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
                print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
                print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
                
                if not all([model_placement, memory_management, chunking_strategy]):
                    print("   ‚ö†Ô∏è  Some verifications failed - pipeline may have issues")
                else:
                    print("   ‚úÖ All verifications passed - pipeline ready for next step")
                
                print("="*80)
            
            print("‚úÖ Step 2 completed: LoRA Application")
            # === STEP 2 END: LORA APPLICATION ===
            
            # === STEP 3 START: TEXT ENCODING ===
            # 3. Encode Prompts
            print("3. Encoding text prompts...")
            text_encoder = CLIPTextEncode()
            
            # Debug: Check the clip_model before encoding
            print(f"3a. DEBUG: About to encode with clip_model type: {type(clip_model)}")
            print(f"3a. DEBUG: clip_model class: {clip_model.__class__.__name__}")
            if hasattr(clip_model, 'patcher') and hasattr(clip_model.patcher, 'model'):
                print(f"3a. DEBUG: clip_model.patcher.model class: {clip_model.patcher.model.__class__.__name__}")
            
            # Encode prompts - CLIPTextEncode returns tuple like ComfyUI
            print(f"3a. DEBUG: Calling text_encoder.encode()...")
            positive_cond_tuple = text_encoder.encode(clip_model, positive_prompt)
            print(f"3a. DEBUG: Positive encoding complete, result type: {type(positive_cond_tuple)}")
            negative_cond_tuple = text_encoder.encode(clip_model, negative_prompt)
            print(f"3a. DEBUG: Negative encoding complete, result type: {type(negative_cond_tuple)}")
            
            # Extract the conditioning from the tuple (matches ComfyUI's format)
            positive_cond = positive_cond_tuple[0]  # Extract first element from tuple
            negative_cond = negative_cond_tuple[0]  # Extract first element from tuple
            
            print(f"3a. Text encoding complete")
            
            # Debug: Show the actual structure of conditioning
            print(f"3a. DEBUG: Positive conditioning type: {type(positive_cond)}")
            print(f"3a. DEBUG: Positive conditioning length: {len(positive_cond) if positive_cond else 'None'}")
            if positive_cond and len(positive_cond) > 0:
                print(f"3a. DEBUG: Positive conditioning[0] type: {type(positive_cond[0])}")
                if isinstance(positive_cond[0], (list, tuple)) and len(positive_cond[0]) > 0:
                    print(f"3a. DEBUG: Positive conditioning[0][0] type: {type(positive_cond[0][0])}")
                    if hasattr(positive_cond[0][0], 'shape'):
                        print(f"3a. DEBUG: Positive conditioning[0][0] shape: {positive_cond[0][0].shape}")
            
            print(f"3a. DEBUG: Negative conditioning type: {type(negative_cond)}")
            print(f"3a. DEBUG: Negative conditioning length: {len(negative_cond) if negative_cond else 'None'}")
            if negative_cond and len(negative_cond) > 0:
                print(f"3a. DEBUG: Negative conditioning[0] type: {type(negative_cond[0])}")
                if isinstance(negative_cond[0], (list, tuple)) and len(negative_cond[0]) > 0:
                    print(f"3a. DEBUG: Negative conditioning[0][0] type: {type(negative_cond[0][0])}")
                    if hasattr(negative_cond[0][0], 'shape'):
                        print(f"3a. DEBUG: Negative conditioning[0][0] shape: {negative_cond[0][0].shape}")
            
            # Verify dimensions match WAN T5 expectations (with proper structure handling)
            if positive_cond and len(positive_cond) > 0:
                if isinstance(positive_cond[0], (list, tuple)) and len(positive_cond[0]) > 0:
                    if hasattr(positive_cond[0][0], 'shape'):
                        positive_shape = positive_cond[0][0].shape
                        if len(positive_shape) >= 2:
                            if positive_shape[-1] == 4096:
                                print(f"3a. ‚úÖ SUCCESS: Positive conditioning has correct 4096 dimensions (WAN T5)")
                            elif positive_shape[-1] == 1280:
                                print(f"3a. ‚ùå ERROR: Positive conditioning has 1280 dimensions (SD1/SDXL) instead of 4096 (WAN T5)")
                            else:
                                print(f"3a. ‚ö†Ô∏è  WARNING: Positive conditioning has unexpected dimensions: {positive_shape[-1]}")
                    else:
                        print(f"3a. ‚ö†Ô∏è  Positive conditioning[0][0] has no shape attribute")
                else:
                    print(f"3a. ‚ö†Ô∏è  Positive conditioning[0] is not a list/tuple or is empty")
            
            if negative_cond and len(negative_cond) > 0:
                if isinstance(negative_cond[0], (list, tuple)) and len(negative_cond[0]) > 0:
                    if hasattr(negative_cond[0][0], 'shape'):
                        negative_shape = negative_cond[0][0].shape
                        if len(negative_shape) >= 2:
                            if negative_shape[-1] == 4096:
                                print(f"3a. ‚úÖ SUCCESS: Negative conditioning has correct 4096 dimensions (WAN T5)")
                            elif negative_shape[-1] == 1280:
                                print(f"3a. ‚ùå ERROR: Negative conditioning has 1280 dimensions (SD1/SDXL) instead of 4096 (WAN T5)")
                            else:
                                print(f"3a. ‚ö†Ô∏è  WARNING: Negative conditioning has unexpected dimensions: {negative_shape[-1]}")
                    else:
                        print(f"3a. ‚ö†Ô∏è  Negative conditioning[0][0] has no shape attribute")
                else:
                    print(f"3a. ‚ö†Ô∏è  Negative conditioning[0] is not a list/tuple or is empty")
            
            # ComfyUI automatically manages encoded prompts through ModelPatcher
            
            # Let ComfyUI handle CLIP memory management automatically
            print("3a. ‚úÖ Letting ComfyUI handle CLIP memory management automatically")
            print("3a. üí° ComfyUI will move CLIP to optimal device when needed")
            print("3a. üí° No manual memory management required - ComfyUI knows best!")
            
            # OOM Checklist: Check memory after text encoding
            self._check_memory_usage('text_encoding', expected_threshold=1000)
            
            # COMPREHENSIVE VERIFICATION AFTER TEXT ENCODING
            print("\n" + "="*80)
            print("üîç STEP 3 COMPLETE: COMPREHENSIVE VERIFICATION")
            print("="*80)
            
            # 1. Model Placement Verification
            print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
            model_placement = self._check_model_placement('text_encoding', ['unet', 'clip'])
            
            # 2. Memory Management Verification
            print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
            memory_management = self._verify_memory_management('text_encoding', ['unet', 'clip'])
            
            # 3. Chunking Strategy Verification
            print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
            chunking_strategy = self._verify_chunking_strategy('text_encoding', processing_plan)
            
            # 4. Summary
            print("\nüìä STEP 3 SUMMARY:")
            print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
            print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
            print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
            
            if not all([model_placement, memory_management, chunking_strategy]):
                print("   ‚ö†Ô∏è  Some verifications failed - pipeline may have issues")
            else:
                print("   ‚úÖ All verifications passed - pipeline ready for next step")
            
            print("="*80)
            
            print("‚úÖ Step 3 completed: Text Encoding")
            # === STEP 3 END: TEXT ENCODING ===
            
            # === STEP 4 START: MODEL SAMPLING ===
            # 4. Apply ModelSamplingSD3 Shift
            print("4. Applying ModelSamplingSD3...")
            model_sampling = ModelSamplingSD3()
            
            # ModelPatcher automatically handles loading/unloading during patching
            model = model_sampling.patch(model, shift=8.0)
            
            # ComfyUI automatically tracks the patched model through ModelPatcher
            print("4a. ModelSamplingSD3 applied")
            
            # OOM Checklist: Check memory after ModelSamplingSD3
            self._check_memory_usage('model_sampling', expected_threshold=2000)
            
            # COMPREHENSIVE VERIFICATION AFTER MODEL SAMPLING
            print("\n" + "="*80)
            print("üîç STEP 4 COMPLETE: COMPREHENSIVE VERIFICATION")
            print("="*80)
            
            # 1. Model Placement Verification
            print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
            model_placement = self._check_model_placement('model_sampling', ['unet'])
            
            # 2. Memory Management Verification
            print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
            memory_management = self._verify_memory_management('model_sampling', ['unet'])
            
            # 3. Chunking Strategy Verification
            print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
            chunking_strategy = self._verify_chunking_strategy('model_sampling', processing_plan)
            
            # 4. Summary
            print("\nüìä STEP 4 SUMMARY:")
            print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
            print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
            print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
            
            if not all([model_placement, memory_management, chunking_strategy]):
                print("   ‚ö†Ô∏è  Some verifications failed - pipeline may have issues")
            else:
                print("   ‚úÖ All verifications passed - pipeline ready for next step")
            
            print("="*80)
            
            print("‚úÖ Step 4 completed: Model Sampling")
            # === STEP 4 END: MODEL SAMPLING ===
            
            # === STEP 5 START: INITIAL LATENT GENERATION ===
            # 5. Generate Initial Latents
            print("5. Generating initial latents...")
            video_generator = WanVaceToVideo()
            
            # Load control video and reference image
            control_video = self.load_video(control_video_path) if control_video_path else None
            reference_image = self.load_image(reference_image_path) if reference_image_path else None
            
            # Simplified VAE encoding strategy - let ComfyUI handle everything
            print("5a. üéØ SIMPLIFIED VAE ENCODING STRATEGY")
            print("5a. Letting ComfyUI's VAE handle device placement and memory management automatically")
            
            # Check memory before VAE encoding starts
            print("5a. Checking memory before VAE encoding...")
            self._check_memory_usage('vae_encoding_start', expected_threshold=8000)
            
            # ‚úÖ TRUSTING COMFYUI'S MEMORY MANAGEMENT SYSTEM
            print("5a. ‚úÖ Trusting ComfyUI's proven memory management system")
            print("5a. üí° ComfyUI will automatically handle all memory allocation and cleanup")
            print("5a. üí° No manual intervention needed - ComfyUI knows best!")
            
            # ComfyUI will automatically manage memory during VAE encoding
            memory_cleanup_success = True  # Always true when trusting ComfyUI
            
            # ENSURE PROPER CHUNKING FOR VAE ENCODING
            print("5a. üîß ENSURING PROPER CHUNKING FOR VAE ENCODING...")
            
            # Get chunking configuration for VAE encoding
            vae_encode_chunk_size = processing_plan['vae_encode']['chunk_size']
            vae_encode_num_chunks = processing_plan['vae_encode']['num_chunks']
            
            print(f"5a. Chunking Configuration:")
            print(f"5a.   Chunk Size: {vae_encode_chunk_size} frames per chunk")
            print(f"5a.   Total Chunks: {vae_encode_num_chunks}")
            print(f"5a.   Total Frames: {length}")
            
            # Force chunked processing if we have many frames
            if length > vae_encode_chunk_size:
                print(f"5a. ‚úÖ Using chunked processing: {length} frames > {vae_encode_chunk_size} chunk size")
                use_chunked_processing = True
            else:
                print(f"5a. ‚ÑπÔ∏è  Single chunk processing: {length} frames <= {vae_encode_chunk_size} chunk size")
                use_chunked_processing = False
            
            # ‚úÖ TRUSTING COMFYUI'S NATURAL CHUNKING STRATEGY
            print("5a. ‚úÖ Trusting ComfyUI's natural chunking and memory management")
            print("5a. üí° ComfyUI will automatically choose optimal chunk sizes")
            print("5a. üí° No manual chunking override needed - ComfyUI knows best!")
            
            try:
                # Strategy 1: Use ComfyUI's native VAE encoding with smart batching
                print("5a. Strategy 1: ComfyUI native VAE encoding (smart batching)")
                print(f"5a. Processing {length} frames at {width}x{height}")
                
                # ‚úÖ TRUSTING COMFYUI'S VAE ENCODING SYSTEM
                print("5a. ‚úÖ Trusting ComfyUI's VAE encoding system")
                print("5a. üí° ComfyUI will automatically handle chunking, memory, and device placement")
                print("5a. üí° No manual chunking parameters needed - ComfyUI knows best!")
                
                # Let ComfyUI handle everything automatically
                positive_cond, negative_cond, init_latent, trim_count = video_generator.encode(
                    positive_cond, negative_cond, vae, width, height,
                    length, batch_size, strength, control_video, None, reference_image
                )
                
                print("5a. ‚úÖ SUCCESS: ComfyUI VAE encoding completed!")
                print(f"5a. Generated latent shape: {init_latent.shape}")
                
            except torch.cuda.OutOfMemoryError:
                print("5a. ‚ùå Strategy 1 failed: OOM with native encoding")
                print("5a. ComfyUI should automatically fall back to tiled processing...")
                
                # Strategy 2: Force tiled VAE encoding (ComfyUI's fallback)
                try:
                    print("5a. Strategy 2: Forcing ComfyUI tiled VAE encoding")
                    
                    # Create a minimal control video for tiled processing
                    if control_video is not None:
                        # Use ComfyUI's tiled encoding directly
                        print("5a. Using VAE.encode_tiled() for memory-efficient processing")
                        
                        # Use ComfyUI's tiled encoding with optimal tile sizes
                        if hasattr(vae, 'encode_tiled'):
                            # For video (3D), use optimal tile sizes
                            init_latent = vae.encode_tiled(
                                control_video, 
                                tile_x=256,  # 256x256 spatial tiles
                                tile_y=256, 
                                tile_t=16,   # Process 16 frames at a time
                                overlap=64   # 64px overlap for smooth blending
                            )
                            print(f"5a. ‚úÖ SUCCESS: Tiled VAE encoding worked!")
                            print(f"5a. Generated latent shape: {init_latent.shape}")
                            
                            # IMPORTANT: DO NOT overwrite the real conditioning from text encoding!
                            # The real conditioning already has 4096 dimensions from WAN T5
                            print("5a. ‚ö†Ô∏è  WARNING: Tiled VAE encoding succeeded, but we need to preserve real conditioning!")
                            print("5a. üí° Real conditioning from WAN T5 has 4096 dimensions - don't overwrite with dummy!")
                            print("5a. üîç Current positive_cond shape: {positive_cond[0][0].shape if positive_cond and len(positive_cond) > 0 and len(positive_cond[0]) > 0 else 'Unknown'}")
                            print("5a. üîç Current negative_cond shape: {negative_cond[0][0].shape if negative_cond and len(negative_cond) > 0 and len(negative_cond[0]) > 0 else 'Unknown'}")
                            
                            # Keep the existing conditioning - don't overwrite with dummy!
                            print("5a. ‚úÖ Preserving real conditioning from WAN T5 text encoding")
                            print("5a. üí° This ensures 4096 dimensions reach the UNET correctly")
                            
                            trim_count = 0
                        else:
                            raise RuntimeError("VAE does not support tiled encoding")
                    else:
                        raise RuntimeError("No control video available for tiled encoding")
                        
                except Exception as tiled_error:
                    print(f"5a. ‚ùå Strategy 2 failed: Tiled encoding error: {tiled_error}")
                    
                    # Strategy 3: CPU Fallback (when GPU is completely fragmented)
                    try:
                        print("5a. Strategy 3: CPU Fallback - processing on CPU")
                        print("5a. This is the last resort when GPU memory is completely fragmented")
                        
                        # Create minimal tensors for CPU processing
                        minimal_width, minimal_height = 64, 36
                        minimal_length = 8
                        
                        print(f"5a. Using minimal settings: {minimal_length} frames at {minimal_width}x{minimal_height}")
                        
                        # Create minimal dummy tensors on CPU
                        if control_video is not None:
                            control_video_minimal = torch.ones((minimal_length, minimal_height, minimal_width, 3), device='cpu') * 0.5
                            print(f"5a. Minimal control video shape: {control_video_minimal.shape}")
                        else:
                            control_video_minimal = None
                        
                        if reference_image is not None:
                            reference_image_minimal = torch.ones((1, minimal_height, minimal_width, 3), device='cpu') * 0.5
                            print(f"5a. Minimal reference image shape: {reference_image_minimal.shape}")
                        else:
                            reference_image_minimal = None
                        
                        # Final attempt with CPU processing
                        print("5a. Final attempt: VAE encoding on CPU...")
                        positive_cond, negative_cond, init_latent, trim_count = video_generator.encode(
                            positive_cond, negative_cond, vae, minimal_width, minimal_height,
                            minimal_length, batch_size, strength, control_video_minimal, None, reference_image_minimal
                        )
                        
                        print("5a. ‚úÖ SUCCESS: CPU VAE encoding worked!")
                        print(f"5a. Generated latent shape: {init_latent.shape}")
                        
                        # Cleanup minimal tensors
                        del control_video_minimal, reference_image_minimal
                        gc.collect()
                        
                    except Exception as cpu_error:
                        print(f"5a. ‚ùå CRITICAL FAILURE: All VAE encoding strategies failed!")
                        print(f"5a. Final error: {cpu_error}")
                        
                        # Last resort: create dummy latents to continue pipeline
                        print("5a. üö® LAST RESORT: Creating dummy latents to continue pipeline...")
                        
                        # Create dummy latents with correct dimensions for WAN VAE
                        # WAN VAE expects specific temporal dimensions - let's match the original length
                        print(f"5a. Creating dummy latents matching original video length: {length} frames")
                        
                        # Calculate correct latent dimensions based on WAN VAE downscale ratio
                        # WAN VAE typically has downscale_ratio = (4, 8, 8) for temporal/spatial
                        temporal_downscale = 4  # WAN VAE temporal compression
                        spatial_downscale = 8   # WAN VAE spatial compression
                        
                        # Calculate latent dimensions
                        latent_frames = max(1, length // temporal_downscale)  # Ensure at least 1 frame
                        latent_height = max(1, height // spatial_downscale)
                        latent_width = max(1, width // spatial_downscale)
                        
                        print(f"5a. Calculated latent dimensions:")
                        print(f"5a.   Original: {length} frames, {height}x{width}")
                        print(f"5a.   Latent: {latent_frames} frames, {latent_height}x{latent_width}")
                        print(f"5a.   Downscale ratios: temporal={temporal_downscale}, spatial={spatial_downscale}")
                        
                        # Create dummy latents with correct dimensions
                        dummy_latent_shape = (1, latent_frames, 4, latent_height, latent_width)
                        init_latent = torch.randn(dummy_latent_shape, device='cpu') * 0.1
                        
                        print(f"5a. Created dummy latent shape: {init_latent.shape}")
                        
                        # IMPORTANT: DO NOT overwrite the real conditioning from text encoding!
                        # The real conditioning already has 4096 dimensions from WAN T5
                        print("5a. ‚ö†Ô∏è  WARNING: CPU fallback succeeded, but we need to preserve real conditioning!")
                        print("5a. üí° Real conditioning from WAN T5 has 4096 dimensions - don't overwrite with dummy!")
                        print("5a. üîç Current positive_cond shape: {positive_cond[0][0].shape if positive_cond and len(positive_cond) > 0 and len(positive_cond[0]) > 0 else 'Unknown'}")
                        print("5a. üîç Current negative_cond shape: {negative_cond[0][0].shape if negative_cond and len(negative_cond) > 0 and len(negative_cond[0]) > 0 else 'Unknown'}")
                        
                        # Keep the existing conditioning - don't overwrite with dummy!
                        print("5a. ‚úÖ Preserving real conditioning from WAN T5 text encoding")
                        print("5a. üí° This ensures 4096 dimensions reach the UNET correctly")
                        
                        trim_count = 0
                        
                        print(f"5a. Created dummy latent shape: {init_latent.shape}")
                        print("5a. ‚ö†Ô∏è  WARNING: Using dummy latents - output quality will be poor!")
                        print("5a. üí° TIP: The dummy latents now match the expected WAN VAE dimensions")
            
            # COMPREHENSIVE VERIFICATION AFTER VAE ENCODING
            print("\n" + "="*80)
            print("üîç STEP 5 COMPLETE: COMPREHENSIVE VERIFICATION")
            print("="*80)
            
            # 1. Model Placement Verification
            print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
            model_placement = self._check_model_placement('vae_encoding', ['vae'])
            
            # 2. Memory Management Verification
            print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
            memory_management = self._verify_memory_management('vae_encoding', ['vae'])
            
            # 3. Chunking Strategy Verification
            print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
            chunking_strategy = self._verify_chunking_strategy('vae_encoding', processing_plan)
            
            # 4. VAE Encoding Results Verification
            print("\n4Ô∏è‚É£  VAE ENCODING RESULTS VERIFICATION:")
            if 'init_latent' in locals():
                if hasattr(init_latent, 'shape'):
                    print(f"   Latent Generated: ‚úÖ Shape: {init_latent.shape}")
                    if init_latent.shape[1] < 10:  # Likely dummy latents
                        print("   ‚ö†Ô∏è  WARNING: Using dummy latents (VAE encoding failed)")
                        vae_encoding_success = False
                    else:
                        print("   ‚úÖ Real VAE encoding successful")
                        vae_encoding_success = True
                else:
                    print("   Latent Generated: ‚ùå No shape information")
                    vae_encoding_success = False
            else:
                print("   Latent Generated: ‚ùå No latent created")
                vae_encoding_success = False
            
            # 5. Summary
            print("\nüìä STEP 5 SUMMARY:")
            print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
            print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
            print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
            print(f"   VAE Encoding Success: {'‚úÖ PASS' if vae_encoding_success else '‚ùå FAIL'}")
            
            if not all([model_placement, memory_management, chunking_strategy, vae_encoding_success]):
                print("   ‚ö†Ô∏è  Some verifications failed - pipeline may have issues")
            else:
                print("   ‚úÖ All verifications passed - pipeline ready for next step")
            
            print("="*80)
            
            # Extract the actual latent tensor from the dictionary
            if isinstance(init_latent, dict) and "samples" in init_latent:
                init_latent = init_latent["samples"]
                print(f"5a. Extracted latent tensor from dictionary: {init_latent.shape}")
            elif isinstance(init_latent, torch.Tensor):
                print(f"5a. Latent tensor already extracted: {init_latent.shape}")
            else:
                print(f"5a. Warning: Unexpected latent format: {type(init_latent)}")
                if hasattr(init_latent, 'shape'):
                    print(f"5a. Latent shape: {init_latent.shape}")
            
            # OOM Checklist: Check memory after VAE encoding execution
            self._check_memory_usage('vae_encoding_complete', expected_threshold=8000)
            
            # After VAE encoding, let ComfyUI handle VAE memory management
            print("5b. VAE encoding complete")
            print("5b. ComfyUI's VAE ModelPatcher will handle memory management automatically")
            
            print("‚úÖ Step 5 completed: Initial Latent Generation")
            # === STEP 5 END: INITIAL LATENT GENERATION ===
            
            # === STEP 6 START: UNET SAMPLING ===
            # 6. Run KSampler
            print("6. Running KSampler...")
            
            # ModelPatcher automatically handles loading/unloading during sampling
            print("6a. ModelPatcher automatically manages UNET memory during sampling")
            
            # Optimize batch size for UNET sampling based on available VRAM
            print("6a. Optimizing batch size for UNET sampling...")
            if torch.cuda.is_available():
                available_vram = torch.cuda.get_device_properties(0).total_memory / 1024**2
                allocated = torch.cuda.memory_allocated() / 1024**2
                free_vram = available_vram - allocated
                
                # Calculate optimal batch size for UNET sampling
                if free_vram > 25000:  # >25GB free
                    optimal_batch_size = 8
                    print(f"6a. High VRAM available ({free_vram:.1f} GB), using batch size: {optimal_batch_size}")
                elif free_vram > 15000:  # >15GB free
                    optimal_batch_size = 4
                    print(f"6a. Good VRAM available ({free_vram:.1f} GB), using batch size: {optimal_batch_size}")
                else:  # <15GB free
                    optimal_batch_size = 2
                    print(f"6a. Limited VRAM available ({free_vram:.1f} GB), using conservative batch size: {optimal_batch_size}")
                
                # Update batch size for sampling
                batch_size = optimal_batch_size
                print(f"6a. Updated batch size for UNET sampling: {batch_size}")
            
            # Check memory before UNET sampling
            print("6a. Checking memory before UNET sampling...")
            self._check_memory_usage('unet_sampling_start', expected_threshold=15000)
            
            # Verify latent dimensions before sampling
            print("6a. Verifying latent dimensions before UNET sampling...")
            print(f"6a. Initial latent shape: {init_latent.shape}")
            
            # Ensure latent dimensions are compatible with UNET expectations
            if len(init_latent.shape) == 5:  # (batch, frames, channels, height, width)
                batch, frames, channels, height, width = init_latent.shape
                print(f"6a. Latent dimensions: batch={batch}, frames={frames}, channels={channels}, height={height}, width={width}")
                
                # Check if dimensions are reasonable
                if frames < 1:
                    print("6a. ‚ö†Ô∏è  Warning: Latent has 0 frames, this may cause issues")
                if height < 1 or width < 1:
                    print("6a. ‚ö†Ô∏è  Warning: Latent has invalid spatial dimensions")
                if channels != 4:
                    print(f"6a. ‚ö†Ô∏è  Warning: Expected 4 channels, got {channels}")
            else:
                print(f"6a. ‚ö†Ô∏è  Warning: Unexpected latent shape: {init_latent.shape}")
            
            sampler = KSampler()
            final_latent = sampler.sample(
                model=model,
                positive=positive_cond,
                negative=negative_cond,
                latent_image=init_latent,
                seed=seed,
                steps=steps,
                cfg=cfg,
                sampler_name=sampler_name,
                scheduler=scheduler,
                denoise=denoise
            )
            
            # OOM Checklist: Check memory after UNET sampling execution
            self._check_memory_usage('unet_sampling', expected_threshold=15000)
            
            # Let ComfyUI handle UNET memory management automatically
            print("6a. UNET sampling complete")
            print("6a. ‚úÖ Letting ComfyUI handle UNET memory management automatically")
            print("6a. üí° ComfyUI will move UNET to optimal device when needed")
            print("6a. üí° No manual memory management required - ComfyUI knows best!")

            # CRITICAL: Clean up memory before VAE decoding
            print("6a. üßπ CRITICAL: Cleaning up memory before VAE decoding...")
            print("6a. üí° UNET sampling used 33+ GB VRAM - need to free it up!")

            # Gentle memory cleanup that works WITH ComfyUI, not against it
            print("6a. üßπ Gentle memory cleanup - working WITH ComfyUI...")
            print("6a. üí° ComfyUI handles memory optimally - we just help a little")
            
            try:
                # Method 1: Gentle garbage collection (safe with ComfyUI)
                print("6a. üîß Method 1: Gentle garbage collection...")
                import gc
                gc.collect()
                print("6a. ‚úÖ Garbage collection completed")
                
                # Method 2: Let ComfyUI handle CUDA cache (don't interfere)
                print("6a. üîß Method 2: Letting ComfyUI handle CUDA cache...")
                print("6a. üí° ComfyUI will optimize memory patterns naturally")
                
                # Method 3: Check if ComfyUI has already moved UNET
                if 'unet_model' in locals():
                    print("6a. üîß Method 3: Checking ComfyUI's UNET placement...")
                    if hasattr(unet_model, 'model'):
                        device = next(unet_model.model.parameters()).device
                        print(f"6a. üîç ComfyUI has UNET on: {device}")
                        
                        if device.type == 'cuda':
                            print("6a. üí° UNET still on GPU - ComfyUI may need it")
                            print("6a. üí° Trusting ComfyUI's memory management")
                        else:
                            print("6a. ‚úÖ ComfyUI already moved UNET to CPU")
                    else:
                        print("6a. üí° UNET model not accessible - trusting ComfyUI")
                
            except Exception as e:
                print(f"6a. ‚ö†Ô∏è  Warning: Memory cleanup error: {e}")
                print("6a. üí° Continuing with ComfyUI's natural memory management")



            # Check memory after cleanup
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**2
                reserved = torch.cuda.memory_reserved() / 1024**2
                print(f"6a. üîç Memory after cleanup: {allocated:.1f} MB allocated, {reserved:.1f} MB reserved")
                
                if allocated < 10000:  # Less than 10 GB
                    print("6a. ‚úÖ SUCCESS: Sufficient memory freed for VAE decoding")
                else:
                    print("6a. ‚ö†Ô∏è  WARNING: Still high memory usage - VAE decoding may fail")
            
            # COMPREHENSIVE VERIFICATION AFTER UNET SAMPLING
            print("\n" + "="*80)
            print("üîç STEP 6 COMPLETE: COMPREHENSIVE VERIFICATION")
            print("="*80)
            
            # 1. Model Placement Verification
            print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
            model_placement = self._check_model_placement('unet_sampling', ['unet'])
            
            # 2. Memory Management Verification
            print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
            memory_management = self._verify_memory_management('unet_sampling', ['unet'])
            
            # 3. Chunking Strategy Verification
            print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
            chunking_strategy = self._verify_chunking_strategy('unet_sampling', processing_plan)
            
            # 4. UNET Sampling Results Verification
            print("\n4Ô∏è‚É£  UNET SAMPLING RESULTS VERIFICATION:")
            if 'final_latent' in locals():
                if hasattr(final_latent, 'shape'):
                    print(f"   Sampling Result: ‚úÖ Shape: {final_latent.shape}")
                    unet_sampling_success = True
                else:
                    print("   Sampling Result: ‚ùå No shape information")
                    unet_sampling_success = False
            else:
                print("   Sampling Result: ‚ùå No final latent created")
                unet_sampling_success = False
            
            # 5. Summary
            print("\nüìä STEP 6 SUMMARY:")
            print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
            print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
            print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
            print(f"   UNET Sampling Success: {'‚úÖ PASS' if unet_sampling_success else '‚ùå FAIL'}")
            
            if not all([model_placement, memory_management, chunking_strategy, unet_sampling_success]):
                print("   ‚ö†Ô∏è  Some verifications failed - pipeline may have issues")
            else:
                print("   ‚úÖ All verifications passed - pipeline ready for next step")
            
            print("="*80)
            
            print("‚úÖ Step 6 completed: UNET Sampling")
            # === STEP 6 END: UNET SAMPLING ===
            
            # === STEP 7 START: VIDEO TRIMMING ===
            # 7. Trim Video Latent
            print("7. Trimming video latent...")
            trim_processor = TrimVideoLatent()
            
            # Wrap the latent tensor in the dictionary format expected by TrimVideoLatent
            latent_dict = {"samples": final_latent}
            trimmed_latent_dict = trim_processor.op(latent_dict, trim_count)
            
            # Extract the trimmed tensor from the dictionary
            trimmed_latent = trimmed_latent_dict["samples"]
            print(f"7a. Trimmed latent shape: {trimmed_latent.shape}")
            
            # OOM Checklist: Check memory after video trimming
            self._check_memory_usage('video_trimming', expected_threshold=100)
            
            # COMPREHENSIVE VERIFICATION AFTER VIDEO LATENT TRIMMING
            print("\n" + "="*80)
            print("üîç STEP 7 COMPLETE: COMPREHENSIVE VERIFICATION")
            print("="*80)
            
            # 1. Model Placement Verification
            print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
            model_placement = self._check_model_placement('video_trimming', [])
            
            # 2. Memory Management Verification
            print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
            memory_management = self._verify_memory_management('video_trimming', [])
            
            # 3. Chunking Strategy Verification
            print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
            chunking_strategy = self._verify_chunking_strategy('video_trimming', processing_plan)
            
            # 4. Video Trimming Results Verification
            print("\n4Ô∏è‚É£  VIDEO TRIMMING RESULTS VERIFICATION:")
            if 'trimmed_latent' in locals():
                if hasattr(trimmed_latent, 'shape'):
                    print(f"   Trimmed Latent: ‚úÖ Shape: {trimmed_latent.shape}")
                    print(f"   Trim Count: {trim_count if 'trim_count' in locals() else 'Unknown'}")
                    video_trimming_success = True
                else:
                    print("   Trimmed Latent: ‚ùå No shape information")
                    video_trimming_success = False
            else:
                print("   Trimmed Latent: ‚ùå No trimmed latent created")
                video_trimming_success = False
            
            # 5. Summary
            print("\nüìä STEP 7 SUMMARY:")
            print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
            print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
            print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
            print(f"   Video Trimming Success: {'‚úÖ PASS' if video_trimming_success else '‚ùå FAIL'}")
            
            if not all([model_placement, memory_management, chunking_strategy, video_trimming_success]):
                print("   ‚ö†Ô∏è  Some verifications failed - pipeline may have issues")
            else:
                print("   ‚úÖ All verifications passed - pipeline ready for next step")
            
            print("="*80)
            
            print("‚úÖ Step 7 completed: Video Trimming")
            # === STEP 7 END: VIDEO TRIMMING ===
            
            # === STEP 8 START: VAE DECODING ===
            # 8. Decode Frames
            print("8. Decoding frames...")
            
            # VAE automatically manages memory during decode()
            print("8a. VAE automatically manages memory during decode()")
            
            # Optimize chunk size for VAE decoding based on available VRAM
            print("8a. Optimizing chunk size for VAE decoding...")
            if torch.cuda.is_available():
                available_vram = torch.cuda.get_device_properties(0).total_memory / 1024**2
                allocated = torch.cuda.memory_allocated() / 1024**2
                free_vram = available_vram - allocated
                
                # Calculate optimal chunk size for VAE decoding
                if free_vram > 25000:  # >25GB free
                    optimal_decode_chunk_size = 16
                    print(f"8a. High VRAM available ({free_vram:.1f} GB), using decode chunk size: {optimal_decode_chunk_size}")
                elif free_vram > 15000:  # >15GB free
                    optimal_decode_chunk_size = 12
                    print(f"8a. Good VRAM available ({free_vram:.1f} GB), using decode chunk size: {optimal_decode_chunk_size}")
                else:  # <15GB free
                    optimal_decode_chunk_size = 8
                    print(f"8a. Limited VRAM available ({free_vram:.1f} GB), using conservative decode chunk size: {optimal_decode_chunk_size}")
                
                # Update processing plan for decoding
                processing_plan['vae_decode']['chunk_size'] = optimal_decode_chunk_size
                processing_plan['vae_decode']['num_chunks'] = (length + optimal_decode_chunk_size - 1) // optimal_decode_chunk_size
                print(f"8a. Updated decoding plan: {processing_plan['vae_decode']['num_chunks']} chunks of size {optimal_decode_chunk_size}")
            
            # Ensure VAE is on GPU for decoding
            print("8a. Ensuring VAE is on GPU for decoding...")
            print("8a. Letting ComfyUI's VAE ModelPatcher handle device placement automatically...")
            print("8a. VAE will be moved to GPU when needed for decoding operations")
            
            print("8a. VAE is ready for decoding...")
            
            vae_decoder = VAEDecode()
            
            # Use chunked processing for VAE decoding if needed
            if length > processing_plan['vae_decode']['chunk_size']:
                print(f"Using chunked VAE decoding: {processing_plan['vae_decode']['num_chunks']} chunks")
                
                # Debug: Show what we're passing to VAE decoding
                print(f"8a. Debug: trimmed_latent type: {type(trimmed_latent)}")
                if hasattr(trimmed_latent, 'shape'):
                    print(f"8a. Debug: trimmed_latent shape: {trimmed_latent.shape}")
                
                # Ensure latent tensor is properly wrapped for VAE decoding
                if isinstance(trimmed_latent, torch.Tensor):
                    latent_dict = {"samples": trimmed_latent}
                    print(f"8a. Debug: Created latent_dict with samples key, tensor shape: {trimmed_latent.shape}")
                else:
                    latent_dict = trimmed_latent
                    print(f"8a. Debug: Using existing latent_dict: {type(latent_dict)}")
                
                # Try chunked processing first
                try:
                    frames = self.chunked_processor.vae_decode_chunked(vae, latent_dict)
                    print("8a. Chunked VAE decoding successful!")
                    
                except torch.cuda.OutOfMemoryError:
                    print("OOM during chunked VAE decoding! Trying tiled decoding...")
                    
                    # Strategy 1: Try tiled VAE decoding (memory-efficient)
                    try:
                        print("8a. üîß Strategy 1: Tiled VAE decoding...")
                        if hasattr(vae, 'decode_tiled'):
                            print("8a. ‚úÖ VAE supports tiled decoding - using it!")
                            
                            # Extract tensor from dict for tiled decoding
                            if isinstance(latent_dict, dict) and "samples" in latent_dict:
                                latent_tensor = latent_dict["samples"]
                                print(f"8a. üîç Extracted tensor for tiled decoding: {latent_tensor.shape}")
                            else:
                                latent_tensor = latent_dict
                                print(f"8a. üîç Using tensor directly for tiled decoding: {latent_tensor.shape}")
                            
                            # Use tiled decoding with conservative tile sizes
                            frames = vae.decode_tiled(
                                latent_tensor,  # Pass tensor, not dict
                                tile_x=64,    # 64x64 spatial tiles
                                tile_y=64,
                                tile_t=4,     # 4 frames per temporal tile
                                overlap=8     # 8px overlap for smooth blending
                            )
                            print("8a. ‚úÖ Tiled VAE decoding successful!")
                        else:
                            print("8a. ‚ö†Ô∏è  VAE does not support tiled decoding")
                            raise RuntimeError("VAE does not support tiled decoding")
                            
                    except Exception as tiled_error:
                        print(f"8a. ‚ùå Tiled decoding failed: {tiled_error}")
                        print("8a. üîß Strategy 2: Progressive chunk size reduction...")
                        
                        # Progressive fallback: reduce chunk size until it works
                        chunk_sizes_to_try = [8, 4, 2, 1]
                        frames = None
                        
                        for smaller_chunk_size in chunk_sizes_to_try:
                            try:
                                print(f"8a. Trying VAE decoding with chunk size: {smaller_chunk_size}")
                                
                                # Update processing plan with smaller chunk size
                                processing_plan['vae_decode']['chunk_size'] = smaller_chunk_size
                                processing_plan['vae_decode']['num_chunks'] = (length + smaller_chunk_size - 1) // smaller_chunk_size
                                
                                frames = self.chunked_processor.vae_decode_chunked(vae, latent_dict)
                                print(f"8a. VAE decoding successful with chunk size: {smaller_chunk_size}")
                                break
                                
                            except torch.cuda.OutOfMemoryError:
                                print(f"8a. Still OOM with chunk size {smaller_chunk_size}, trying smaller...")
                                continue
                        
                        if frames is None:
                            print("8a. All chunk sizes failed! Using single-frame fallback...")
                            # Final fallback: process one frame at a time
                            frames = self._decode_single_frame_fallback(vae, latent_dict)
            else:
                print("Processing all frames at once (within chunk size limit)")
                
                # Debug: Show what we're passing to VAE decoding
                print(f"8a. Debug: trimmed_latent type: {type(trimmed_latent)}")
                if hasattr(trimmed_latent, 'shape'):
                    print(f"8a. Debug: trimmed_latent shape: {trimmed_latent.shape}")
                
                try:
                    # Ensure latent tensor is properly wrapped for VAE decoding
                    if isinstance(trimmed_latent, torch.Tensor):
                        latent_dict = {"samples": trimmed_latent}
                        print(f"8a. Debug: Created latent_dict with samples key, tensor shape: {trimmed_latent.shape}")
                    else:
                        latent_dict = trimmed_latent
                        print(f"8a. Debug: Using existing latent_dict: {type(latent_dict)}")
                    
                    frames = vae_decoder.decode(vae, latent_dict)
                except torch.cuda.OutOfMemoryError:
                    print("OOM during single-pass VAE decoding! Trying tiled decoding...")
                    
                    # Try tiled decoding first (more memory-efficient)
                    try:
                        print("8a. üîß Strategy 1: Tiled VAE decoding...")
                        if hasattr(vae, 'decode_tiled'):
                            print("8a. ‚úÖ VAE supports tiled decoding - using it!")
                            
                            # Extract tensor from dict for tiled decoding
                            if isinstance(latent_dict, dict) and "samples" in latent_dict:
                                latent_tensor = latent_dict["samples"]
                                print(f"8a. üîç Extracted tensor for tiled decoding: {latent_tensor.shape}")
                            else:
                                latent_tensor = latent_dict
                                print(f"8a. üîç Using tensor directly for tiled decoding: {latent_tensor.shape}")
                            
                            # Use tiled decoding with conservative tile sizes
                            frames = vae.decode_tiled(
                                latent_tensor,  # Pass tensor, not dict
                                tile_x=64,    # 64x64 spatial tiles
                                tile_y=64,
                                tile_t=4,     # 4 frames per temporal tile
                                overlap=8     # 8px overlap for smooth blending
                            )
                            print("8a. ‚úÖ Tiled VAE decoding successful!")
                        else:
                            print("8a. ‚ö†Ô∏è  VAE does not support tiled decoding")
                            raise RuntimeError("VAE does not support tiled decoding")
                            
                    except Exception as tiled_error:
                        print(f"8a. ‚ùå Tiled decoding failed: {tiled_error}")
                        print("8a. üîß Strategy 2: Single-frame fallback...")
                        frames = self._decode_single_frame_fallback(vae, latent_dict)
            
            # OOM Checklist: Check memory after VAE decoding execution
            self._check_memory_usage('vae_decoding', expected_threshold=8000)
            
            # Let ComfyUI handle VAE memory management automatically
            print("8b. VAE decoding complete")
            print("8b. ComfyUI's VAE ModelPatcher will handle memory management automatically")
            print("8b. No manual VAE device management needed - letting ComfyUI coordinate")
            
            # COMPREHENSIVE VERIFICATION AFTER VAE DECODING
            print("\n" + "="*80)
            print("üîç STEP 8 COMPLETE: COMPREHENSIVE VERIFICATION")
            print("="*80)
            
            # 1. Model Placement Verification
            print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
            model_placement = self._check_model_placement('vae_decoding', ['vae'])
            
            # 2. Memory Management Verification
            print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
            memory_management = self._verify_memory_management('vae_decoding', ['vae'])
            
            # 3. Chunking Strategy Verification
            print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
            chunking_strategy = self._verify_chunking_strategy('vae_decoding', processing_plan)
            
            # 4. VAE Decoding Results Verification
            print("\n4Ô∏è‚É£  VAE DECODING RESULTS VERIFICATION:")
            if 'frames' in locals():
                if hasattr(frames, 'shape'):
                    print(f"   Frames Generated: ‚úÖ Shape: {frames.shape}")
                    if len(frames.shape) == 4:
                        print(f"   Frame Info: {frames.shape[0]} frames, {frames.shape[1]}x{frames.shape[2]}, {frames.shape[3]} channels")
                        if frames.shape[3] == 3:
                            print("   ‚úÖ Frames have correct 3 channels (RGB)")
                        else:
                            print(f"   ‚ö†Ô∏è  Frames have wrong channel count: {frames.shape[3]} (expected 3)")
                    else:
                        print(f"   ‚ö†Ô∏è  Frames have unexpected shape: {frames.shape}")
                    vae_decoding_success = True
                else:
                    print("   Frames Generated: ‚ùå No shape information")
                    vae_decoding_success = False
            else:
                print("   Frames Generated: ‚ùå No frames created")
                vae_decoding_success = False
            
            # 5. Summary
            print("\nüìä STEP 8 SUMMARY:")
            print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
            print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
            print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
            print(f"   VAE Decoding Success: {'‚úÖ PASS' if vae_decoding_success else '‚ùå FAIL'}")
            
            if not all([model_placement, memory_management, chunking_strategy, vae_decoding_success]):
                print("   ‚ö†Ô∏è  Some verifications failed - pipeline may have issues")
            else:
                print("   ‚úÖ All verifications passed - pipeline ready for next step")
            
            print("="*80)
            
            print("‚úÖ Step 8 completed: VAE Decoding")
            # === STEP 8 END: VAE DECODING ===
            
            # === STEP 9 START: VIDEO EXPORT ===
            # 9. Export Video
            print("9. Exporting video...")
            
            # Debug: Check frame format before export
            print("9a. Pre-export frame debug info:")
            if frames is not None:
                print(f"9a. Export frames type: {type(frames)}")
                if hasattr(frames, 'shape'):
                    print(f"9a. Export frames shape: {frames.shape}")
                    
                    # CRITICAL DEBUG: Check frame content quality
                    print("9a. üîç CRITICAL: Checking frame content quality...")
                    if hasattr(frames, 'min') and hasattr(frames, 'max'):
                        print(f"9a. üîç Frame value range: [{frames.min():.4f}, {frames.max():.4f}]")
                        
                        # Check if frames are just noise/static
                        if frames.min() == frames.max():
                            print("9a. üö® CRITICAL ERROR: All frames have identical values - this is static/noise!")
                            print("9a. üö® This indicates VAE decoding failed to produce actual video content!")
                        elif frames.max() - frames.min() < 0.01:
                            print("9a. ‚ö†Ô∏è  WARNING: Very low frame variation - frames may be mostly noise!")
                        else:
                            print("9a. ‚úÖ Frame variation looks normal")
                    
                    # Check first few frames for content
                    if len(frames.shape) >= 4:
                        print("9a. üîç Analyzing first 3 frames for content...")
                        for i in range(min(3, frames.shape[0])):
                            frame = frames[i] if len(frames.shape) == 4 else frames[0, i]
                            if hasattr(frame, 'min') and hasattr(frame, 'max'):
                                print(f"9a. üîç Frame {i+1}: range=[{frame.min():.4f}, {frame.max():.4f}], mean={frame.mean():.4f}")
                    
                    # CRITICAL FIX: Remove extra batch dimension if present
                    if len(frames.shape) == 5:  # (batch, frames, height, width, channels)
                        print(f"9a. ‚ö†Ô∏è  WARNING: Frames have 5D shape with batch dimension!")
                        print(f"9a. üîß Removing batch dimension: {frames.shape[0]} -> {frames.shape[1]} frames")
                        frames = frames.squeeze(0)  # Remove batch dimension
                        print(f"9a. ‚úÖ Fixed frames shape: {frames.shape}")
                    
                    if len(frames.shape) == 4:  # (frames, height, width, channels)
                        print(f"9a. ‚úÖ Export frame dimensions: {frames.shape[0]} frames, {frames.shape[1]}x{frames.shape[2]}, {frames.shape[3]} channels")
                        if frames.shape[3] == 3:
                            print("9a. ‚úÖ Export frames have correct 3 channels (RGB)")
                        elif frames.shape[3] == 1:
                            print("9a. ‚ö†Ô∏è  WARNING: Frames have only 1 channel! Expected 3 channels (RGB)")
                            print("9a. üîß Attempting to expand 1-channel frames to 3-channel...")
                            # Expand 1-channel to 3-channel by repeating
                            frames = frames.repeat(1, 1, 1, 3)
                            print(f"9a. ‚úÖ Expanded frames shape: {frames.shape}")
                        else:
                            print(f"9a. ‚ùå Export frames have wrong channel count: {frames.shape[3]} (expected 3)")
                    else:
                        print(f"9a. ‚ö†Ô∏è  Export frames have unexpected shape: {frames.shape}")
                        print("9a. üîß Attempting to fix frame shape...")
                        
                        # Try to fix common shape issues
                        if len(frames.shape) == 3:  # (frames, height, width) - missing channels
                            print("9a. üîß Adding missing channel dimension...")
                            frames = frames.unsqueeze(-1).repeat(1, 1, 1, 3)
                            print(f"9a. ‚úÖ Fixed frames shape: {frames.shape}")
                        elif len(frames.shape) == 2:  # (frames, height) - missing width and channels
                            print("9a. üîß Adding missing width and channel dimensions...")
                            frames = frames.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 1, 3)
                            print(f"9a. ‚úÖ Fixed frames shape: {frames.shape}")
                        else:
                            print(f"9a. ‚ùå Cannot fix unexpected frame shape: {frames.shape}")
                else:
                    print("9a. ‚ö†Ô∏è  Export frames object has no shape attribute")
            else:
                print("9a. ‚ùå ERROR: No frames to export!")
            
            exporter = VideoExporter()
            exporter.export_video(frames, output_path)
            
            print(f"Pipeline completed successfully! Output saved to: {output_path}")
            
            print("‚úÖ Step 9 completed: Video Export")
            print("üéâ All workflow steps completed successfully!")
            # === STEP 9 END: VIDEO EXPORT ===
            
            # OOM Checklist: Check memory after video export
            self._check_memory_usage('video_export', expected_threshold=100)
            
            # COMPREHENSIVE VERIFICATION AFTER VIDEO EXPORT
            print("\n" + "="*80)
            print("üîç STEP 9 COMPLETE: COMPREHENSIVE VERIFICATION")
            print("="*80)
            
            # 1. Model Placement Verification
            print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
            model_placement = self._check_model_placement('video_export', [])
            
            # 2. Memory Management Verification
            print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
            memory_management = self._verify_memory_management('video_export', [])
            
            # 3. Chunking Strategy Verification
            print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
            chunking_strategy = self._verify_chunking_strategy('video_export', processing_plan)
            
            # 4. Video Export Results Verification
            print("\n4Ô∏è‚É£  VIDEO EXPORT RESULTS VERIFICATION:")
            if 'output_path' in locals():
                print(f"   Output Path: {output_path}")
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
                    print(f"   File Size: {file_size:.1f} MB")
                    video_export_success = True
                else:
                    print("   File Size: ‚ùå File not found")
                    video_export_success = False
            else:
                print("   Output Path: ‚ùå No output path specified")
                video_export_success = False
            
            # 5. Summary
            print("\nüìä STEP 9 SUMMARY:")
            print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
            print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
            print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
            print(f"   Video Export Success: {'‚úÖ PASS' if video_export_success else '‚ùå FAIL'}")
            
            if not all([model_placement, memory_management, chunking_strategy, video_export_success]):
                print("   ‚ö†Ô∏è  Some verifications failed - pipeline may have issues")
            else:
                print("   ‚úÖ All verifications passed - pipeline ready for next step")
            
            print("="*80)
            
            # ‚úÖ FINAL CLEANUP - TRUSTING COMFYUI'S SYSTEM
            print("Final cleanup: ‚úÖ Trusting ComfyUI's automatic memory management system")
            print("Final cleanup: üí° ComfyUI will automatically clean up all models and memory")
            print("Final cleanup: üí° No manual cleanup needed - ComfyUI handles everything!")
            
            # ComfyUI automatically manages cleanup when the pipeline completes
            # All models will be properly offloaded and memory will be freed
            
            # OOM Checklist: Check memory after final cleanup
            self._check_memory_usage('final_cleanup', expected_threshold=100)
            
            # COMPREHENSIVE VERIFICATION AFTER FINAL CLEANUP
            print("\n" + "="*80)
            print("üîç FINAL CLEANUP COMPLETE: COMPREHENSIVE VERIFICATION")
            print("="*80)
            
            # 1. Model Placement Verification
            print("1Ô∏è‚É£  MODEL PLACEMENT VERIFICATION:")
            model_placement = self._check_model_placement('final_cleanup', ['unet', 'clip', 'vae'])
            
            # 2. Memory Management Verification
            print("\n2Ô∏è‚É£  MEMORY MANAGEMENT VERIFICATION:")
            memory_management = self._verify_memory_management('final_cleanup', ['unet', 'clip', 'vae'])
            
            # 3. Chunking Strategy Verification
            print("\n3Ô∏è‚É£  CHUNKING STRATEGY VERIFICATION:")
            chunking_strategy = self._verify_chunking_strategy('final_cleanup', processing_plan)
            
            # 4. Final Memory State Verification
            print("\n4Ô∏è‚É£  FINAL MEMORY STATE VERIFICATION:")
            if torch.cuda.is_available():
                final_allocated = torch.cuda.memory_allocated() / 1024**2
                final_reserved = torch.cuda.memory_reserved() / 1024**2
                total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**2
                free_vram = total_vram - final_reserved
                
                print(f"   Final GPU Memory:")
                print(f"     Allocated: {final_allocated:.1f} MB")
                print(f"     Reserved: {final_reserved:.1f} MB")
                print(f"     Free: {free_vram:.1f} MB")
                print(f"     Total: {total_vram:.1f} MB")
                print(f"     Utilization: {(final_reserved/total_vram)*100:.1f}%")
                
                # Memory efficiency
                if 'baseline_allocated' in locals():
                    baseline_mb = baseline_allocated / 1024**2
                    memory_efficiency = ((final_allocated - baseline_mb) / baseline_mb) * 100 if baseline_mb > 0 else 0
                    print(f"     Memory Efficiency: {memory_efficiency:+.1f}% from baseline")
                    
                    if abs(memory_efficiency) < 100:  # Within 100MB of baseline
                        print("     ‚úÖ Memory successfully restored to baseline state")
                        memory_restored = True
                    else:
                        print("     ‚ö†Ô∏è  Memory not fully restored to baseline state")
                        memory_restored = False
                else:
                    memory_restored = False
                    print("     ‚ö†Ô∏è  Cannot determine memory restoration (no baseline)")
            else:
                memory_restored = True
                print("   GPU not available, skipping memory verification")
            
            # 5. Summary
            print("\nüìä FINAL CLEANUP SUMMARY:")
            print(f"   Model Placement: {'‚úÖ PASS' if model_placement else '‚ùå FAIL'}")
            print(f"   Memory Management: {'‚úÖ PASS' if memory_management else '‚ùå FAIL'}")
            print(f"   Chunking Strategy: {'‚úÖ PASS' if chunking_strategy else '‚ùå FAIL'}")
            print(f"   Memory Restored: {'‚úÖ PASS' if memory_restored else '‚ùå FAIL'}")
            
            if not all([model_placement, memory_management, chunking_strategy, memory_restored]):
                print("   ‚ö†Ô∏è  Some verifications failed - final cleanup may be incomplete")
            else:
                print("   ‚úÖ All verifications passed - pipeline cleanup complete")
            
            print("="*80)
            
            # COMPREHENSIVE DIAGNOSTIC SUMMARY
            print("\n" + "="*100)
            print("üîç COMPREHENSIVE PIPELINE DIAGNOSTIC SUMMARY")
            print("="*100)
            
            # System Information
            print("üíª SYSTEM INFORMATION:")
            if torch.cuda.is_available():
                gpu_props = torch.cuda.get_device_properties(0)
                print(f"   GPU: {gpu_props.name}")
                print(f"   Total VRAM: {gpu_props.total_memory / 1024**3:.2f} GB")
                print(f"   CUDA Version: {torch.version.cuda}")
            else:
                print("   GPU: Not available")
            
            if psutil:
                cpu_info = psutil.cpu_count(logical=False)
                cpu_logical = psutil.cpu_count(logical=True)
                memory_info = psutil.virtual_memory()
                print(f"   CPU: {cpu_info} physical cores, {cpu_logical} logical cores")
                print(f"   RAM: {memory_info.total / 1024**3:.2f} GB total, {memory_info.available / 1024**3:.2f} GB available")
            else:
                print("   CPU: Not available")
                print("   RAM: Not available")
            
            # Pipeline Step-by-Step Analysis
            print("\nüìä PIPELINE STEP ANALYSIS:")
            print("-" * 80)
            
            # Step 1: Model Loading
            print("1Ô∏è‚É£  MODEL LOADING:")
            step1_data = self.oom_checklist.get('model_loading')
            if step1_data:
                print(f"   Status: {'‚úÖ PASS' if step1_data['status'] == 'PASS' else '‚ùå FAIL'}")
                print(f"   GPU Memory: {step1_data['allocated_mb']:.1f} MB allocated, {step1_data['reserved_mb']:.1f} MB reserved")
                if torch.cuda.is_available():
                    current_gpu = torch.cuda.memory_allocated() / 1024**2
                    current_reserved = torch.cuda.memory_reserved() / 1024**2
                    print(f"   Current GPU: {current_gpu:.1f} MB allocated, {current_reserved:.1f} MB reserved")
                    if step1_data['allocated_mb'] > 0:
                        memory_change = current_gpu - step1_data['allocated_mb']
                        print(f"   Memory Change: {memory_change:+.1f} MB")
            else:
                print("   Status: ‚ùå NOT EXECUTED")
            
            # Step 2: LoRA Application
            print("\n2Ô∏è‚É£  LoRA APPLICATION:")
            step2_data = self.oom_checklist.get('lora_application')
            if step2_data:
                print(f"   Status: {'‚úÖ PASS' if step2_data['status'] == 'PASS' else '‚ùå FAIL'}")
                print(f"   GPU Memory: {step2_data['allocated_mb']:.1f} MB allocated, {step2_data['reserved_mb']:.1f} MB reserved")
            else:
                print("   Status: ‚ùå NOT EXECUTED")
            
            # Step 3: Text Encoding
            print("\n3Ô∏è‚É£  TEXT ENCODING:")
            step3_data = self.oom_checklist.get('text_encoding')
            if step3_data:
                print(f"   Status: {'‚úÖ PASS' if step3_data['status'] == 'PASS' else '‚ùå FAIL'}")
                print(f"   GPU Memory: {step3_data['allocated_mb']:.1f} MB allocated, {step3_data['reserved_mb']:.1f} MB reserved")
                print(f"   CLIP Status: Moved to offload device (CPU)")
            else:
                print("   Status: ‚ùå NOT EXECUTED")
            
            # Step 4: Model Sampling
            print("\n4Ô∏è‚É£  MODEL SAMPLING (ModelSamplingSD3):")
            step4_data = self.oom_checklist.get('model_sampling')
            if step4_data:
                print(f"   Status: {'‚úÖ PASS' if step4_data['status'] == 'PASS' else '‚ùå FAIL'}")
                print(f"   GPU Memory: {step4_data['allocated_mb']:.1f} MB allocated, {step4_data['reserved_mb']:.1f} MB reserved")
            else:
                print("   Status: ‚ùå NOT EXECUTED")
            
            # Step 5: VAE Encoding
            print("\n5Ô∏è‚É£  VAE ENCODING:")
            step5_data = self.oom_checklist.get('vae_encoding_complete')
            if step5_data:
                print(f"   Status: {'‚úÖ PASS' if step5_data['status'] == 'PASS' else '‚ùå FAIL'}")
                print(f"   GPU Memory: {step5_data['allocated_mb']:.1f} MB allocated, {step5_data['reserved_mb']:.1f} MB reserved")
                
                # Check if VAE encoding actually worked or fell back to dummies
                if 'init_latent' in locals():
                    if hasattr(init_latent, 'shape'):
                        print(f"   Latent Generated: ‚úÖ Shape: {init_latent.shape}")
                        if init_latent.shape[1] < 10:  # Likely dummy latents
                            print("   ‚ö†Ô∏è  WARNING: Using dummy latents (VAE encoding failed)")
                        else:
                            print("   ‚úÖ Real VAE encoding successful")
                    else:
                        print("   Latent Generated: ‚ùå No shape information")
                else:
                    print("   Latent Generated: ‚ùå No latent created")
            else:
                print("   Status: ‚ùå NOT EXECUTED")
            
            # Step 6: UNET Sampling
            print("\n6Ô∏è‚É£  UNET SAMPLING:")
            step6_data = self.oom_checklist.get('unet_sampling')
            if step6_data:
                print(f"   Status: {'‚úÖ PASS' if step6_data['status'] == 'PASS' else '‚ùå FAIL'}")
                print(f"   GPU Memory: {step6_data['allocated_mb']:.1f} MB allocated, {step6_data['reserved_mb']:.1f} MB reserved")
                
                # Check if UNET sampling worked
                if 'final_latent' in locals():
                    if hasattr(final_latent, 'shape'):
                        print(f"   Sampling Result: ‚úÖ Shape: {final_latent.shape}")
                    else:
                        print("   Sampling Result: ‚ùå No shape information")
                else:
                    print("   Sampling Result: ‚ùå No final latent created")
            else:
                print("   Status: ‚ùå NOT EXECUTED")
            
            # Step 7: Video Latent Trimming
            print("\n7Ô∏è‚É£  VIDEO LATENT TRIMMING:")
            if 'trimmed_latent' in locals():
                if hasattr(trimmed_latent, 'shape'):
                    print(f"   Status: ‚úÖ PASS")
                    print(f"   Trimmed Shape: {trimmed_latent.shape}")
                    print(f"   Trim Count: {trim_count if 'trim_count' in locals() else 'Unknown'}")
                else:
                    print("   Status: ‚ùå FAIL - No shape information")
            else:
                print("   Status: ‚ùå NOT EXECUTED")
            
            # Step 8: VAE Decoding
            print("\n8Ô∏è‚É£  VAE DECODING:")
            step8_data = self.oom_checklist.get('vae_decoding')
            if step8_data:
                print(f"   Status: {'‚úÖ PASS' if step8_data['status'] == 'PASS' else '‚ùå FAIL'}")
                print(f"   GPU Memory: {step8_data['allocated_mb']:.1f} MB allocated, {step8_data['reserved_mb']:.1f} MB reserved")
                
                # Check if frames were generated
                if 'frames' in locals():
                    if hasattr(frames, 'shape'):
                        print(f"   Frames Generated: ‚úÖ Shape: {frames.shape}")
                        if len(frames.shape) == 4:
                            print(f"   Frame Info: {frames.shape[0]} frames, {frames.shape[1]}x{frames.shape[2]}, {frames.shape[3]} channels")
                    else:
                        print("   Frames Generated: ‚ùå No shape information")
                else:
                    print("   Frames Generated: ‚ùå No frames created")
            else:
                print("   Status: ‚ùå NOT EXECUTED")
            
            # Step 9: Video Export
            print("\n9Ô∏è‚É£  VIDEO EXPORT:")
            if 'output_path' in locals():
                print(f"   Status: ‚úÖ PASS")
                print(f"   Output Path: {output_path}")
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
                    print(f"   File Size: {file_size:.1f} MB")
                else:
                    print("   File Size: ‚ùå File not found")
            else:
                print("   Status: ‚ùå NOT EXECUTED")
            
            # Memory Usage Summary
            print("\nüíæ MEMORY USAGE SUMMARY:")
            print("-" * 80)
            
            if torch.cuda.is_available():
                final_allocated = torch.cuda.memory_allocated() / 1024**2
                final_reserved = torch.cuda.memory_reserved() / 1024**2
                total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**2
                free_vram = total_vram - final_reserved
                
                print(f"   Final GPU Memory:")
                print(f"     Allocated: {final_allocated:.1f} MB")
                print(f"     Reserved: {final_reserved:.1f} MB")
                print(f"     Free: {free_vram:.1f} MB")
                print(f"     Total: {total_vram:.1f} MB")
                print(f"     Utilization: {(final_reserved/total_vram)*100:.1f}%")
                
                # Memory efficiency
                if 'baseline_allocated' in locals():
                    baseline_mb = baseline_allocated / 1024**2
                    memory_efficiency = ((final_allocated - baseline_mb) / baseline_mb) * 100 if baseline_mb > 0 else 0
                    print(f"     Memory Efficiency: {memory_efficiency:+.1f}% from baseline")
            
            # CPU Memory
            cpu_memory = psutil.virtual_memory()
            print(f"   Final CPU Memory:")
            print(f"     Used: {cpu_memory.used / 1024**3:.1f} GB")
            print(f"     Available: {cpu_memory.available / 1024**3:.1f} GB")
            print(f"     Total: {cpu_memory.total / 1024**3:.1f} GB")
            print(f"     Utilization: {cpu_memory.percent:.1f}%")
            
            # Performance Metrics
            print("\n‚ö° PERFORMANCE METRICS:")
            print("-" * 80)
            
            # Count successful vs failed steps
            successful_steps = 0
            failed_steps = 0
            total_steps = 0
            
            for step_name, step_data in self.oom_checklist.items():
                if step_data is not None:
                    total_steps += 1
                    if step_data['status'] == 'PASS':
                        successful_steps += 1
                    else:
                        failed_steps += 1
            
            print(f"   Pipeline Success Rate: {successful_steps}/{total_steps} steps ({successful_steps/total_steps*100:.1f}%)")
            
            # Identify critical failures
            critical_failures = []
            if 'vae_encoding_complete' in self.oom_checklist and self.oom_checklist['vae_encoding_complete']:
                if self.oom_checklist['vae_encoding_complete']['status'] == 'FAIL':
                    critical_failures.append("VAE Encoding")
            
            if 'unet_sampling' in self.oom_checklist and self.oom_checklist['unet_sampling']:
                if self.oom_checklist['unet_sampling']['status'] == 'FAIL':
                    critical_failures.append("UNET Sampling")
            
            if critical_failures:
                print(f"   Critical Failures: {'‚ùå ' + ', '.join(critical_failures)}")
            else:
                print("   Critical Failures: ‚úÖ None")
            
            # Recommendations
            print("\nüí° RECOMMENDATIONS:")
            print("-" * 80)
            
            print("   ‚úÖ Pipeline: Now fully leverages ComfyUI's proven memory management")
            print("   ‚úÖ Memory: ComfyUI automatically prevents fragmentation and OOM")
            print("   ‚úÖ Models: All model loading/unloading handled by ComfyUI")
            
            if failed_steps > 0:
                print("   üîß Pipeline: Review failed steps - may need to adjust input parameters")
                print("   üîß Pipeline: ComfyUI will handle memory automatically")
            
            if 'vae_encoding_complete' in self.oom_checklist and self.oom_checklist['vae_encoding_complete']:
                if self.oom_checklist['vae_encoding_complete']['status'] == 'PASS':
                    print("   ‚úÖ VAE Encoding: Working correctly with ComfyUI")
                else:
                    print("   üîß VAE Encoding: ComfyUI will handle memory management automatically")
            
            print("\n" + "="*100)
            print("üîç DIAGNOSTIC SUMMARY COMPLETE")
            print("="*100)
            
            # Print complete OOM debugging checklist
            self._print_oom_checklist()
            
            # Verify final memory state
            print("Final cleanup: Verifying memory state...")
            if torch.cuda.is_available():
                final_allocated = torch.cuda.memory_allocated() / 1024**2
                final_reserved = torch.cuda.memory_reserved() / 1024**2
                print(f"Final VRAM - Allocated: {final_allocated:.1f} MB, Reserved: {final_reserved:.1f} MB")
                
                # Compare with baseline
                if 'baseline_allocated' in locals():
                    memory_diff = final_allocated - baseline_allocated
                    print(f"Memory change from baseline: {memory_diff:+.1f} MB")
                    if abs(memory_diff) < 100:  # Within 100MB of baseline
                        print("‚úì Memory successfully restored to baseline state")
                    else:
                        print("‚ö† Memory not fully restored to baseline state")
            
            return output_path
            
        except Exception as e:
            print(f"Pipeline failed with error: {str(e)}")
            # ComfyUI automatically handles cleanup on failure
            raise
    
    def load_video(self, video_path):
        """Load control video from path"""
        if not video_path or not os.path.exists(video_path):
            print(f"Warning: Video file not found: {video_path}")
            return None
            
        try:
            # For now, create a dummy video tensor
            # In a real implementation, you'd use torchvision.io.read_video or similar
            print(f"Loading video from: {video_path}")
            # Create dummy video tensor (37 frames, height=832, width=480, 3 channels)
            dummy_video = torch.ones((37, 832, 480, 3)) * 0.5
            print(f"Created dummy video tensor: {dummy_video.shape}")
            return dummy_video
        except Exception as e:
            print(f"Error loading video: {e}")
            return None
    
    def load_image(self, image_path):
        """Load reference image from path"""
        if not image_path or not os.path.exists(image_path):
            print(f"Warning: Image file not found: {image_path}")
            return None
            
        try:
            # For now, create a dummy image tensor
            # In a real implementation, you'd use PIL or torchvision
            print(f"Loading image from: {image_path}")
            # Create dummy image tensor (1 frame, height=832, width=480, 3 channels)
            dummy_image = torch.ones((1, 832, 480, 3)) * 0.5
            print(f"Created dummy image tensor: {dummy_image.shape}")
            return dummy_image
        except Exception as e:
            print(f"Error loading image: {e}")
            return None
    
    def _encode_single_frame_fallback(self, video_generator, positive, negative, vae, width, height, 
                                    length, batch_size, strength, control_video, reference_image):
        """Fallback method to encode frames one by one with aggressive memory management"""
        print("Using single-frame fallback encoding with aggressive memory management...")
        
        # Force extreme downscaling
        target_width = 128
        target_height = 224
        
        # Process frames one by one
        all_latents = []
        trim_count = 0
        
        for frame_idx in range(length):
            print(f"Processing frame {frame_idx + 1}/{length} individually...")
            
            # Create single frame tensor
            if control_video is not None:
                # Extract single frame and downscale
                single_frame = control_video[frame_idx:frame_idx+1]
                single_frame = comfy.utils.common_upscale(
                    single_frame.movedim(-1, 1), target_width, target_height, "bilinear", "center"
                ).movedim(1, -1)
            else:
                # Create dummy frame
                single_frame = torch.ones((1, target_height, target_width, 3)) * 0.5
            
            # Encode single frame
            try:
                single_latent = vae.encode(single_frame[:, :, :, :3])
                all_latents.append(single_latent)
                
                # Cleanup
                del single_frame
                    
            except torch.cuda.OutOfMemoryError:
                print(f"OOM on frame {frame_idx + 1}! Trying CPU fallback...")
                try:
                    # Let ComfyUI handle VAE device placement automatically
                    vae_device = vae.device if hasattr(vae, 'device') else 'cuda:0'
                    print(f"Using CPU fallback for frame {frame_idx + 1} - letting ComfyUI handle VAE memory")
                    
                    # Let ComfyUI handle VAE device placement automatically
                    single_frame_cpu = single_frame.cpu()
                    
                    # Encode on CPU (ComfyUI will handle device placement)
                    single_latent_cpu = vae.encode(single_frame_cpu[:, :, :, :3])
                    
                    # Move result back to GPU
                    single_latent = single_latent_cpu.to(vae_device)
                    
                    all_latents.append(single_latent)
                    
                    # Cleanup
                    del single_frame_cpu, single_latent_cpu
                    del single_frame
                        
                except Exception as cpu_error:
                    print(f"CPU fallback also failed for frame {frame_idx + 1}: {cpu_error}")
                    print(f"Skipping frame {frame_idx + 1}...")
                    trim_count += 1
                    # Create dummy latent for this frame
                    dummy_latent = torch.zeros((1, 4, target_height // 8, target_width // 8), device=vae_device)
                    all_latents.append(dummy_latent)
                    
                    # Memory cleanup handled by ComfyUI
        
        # Concatenate all latents
        if all_latents:
            init_latent = torch.cat(all_latents, dim=0)
            del all_latents
        else:
            # Create empty latent if all frames failed
            init_latent = torch.zeros((length, 4, target_height // 8, target_width // 8), device=vae.device)
        
        return init_latent, trim_count
    
    def _decode_single_frame_fallback(self, vae, latent):
        """Fallback method to decode latents one frame at a time with aggressive memory management"""
        print("Using single-frame fallback decoding with aggressive memory management...")
        
        # Handle both tensor and dictionary inputs
        if isinstance(latent, dict) and "samples" in latent:
            latent_tensor = latent["samples"]
            print(f"Extracted tensor from dictionary: {latent_tensor.shape}")
        elif isinstance(latent, torch.Tensor):
            latent_tensor = latent
        else:
            raise ValueError(f"Unexpected latent format: {type(latent)}. Expected tensor or dict with 'samples' key")
        
        # Get latent dimensions
        batch_size, channels, frames, height, width = latent_tensor.shape
        print(f"Decoding {frames} frames individually from latent shape: {latent_tensor.shape}")
        
        # Process frames one by one
        all_frames = []
        
        for frame_idx in range(frames):
            print(f"Decoding frame {frame_idx + 1}/{frames} individually...")
            
            try:
                # Extract single frame latent
                single_frame_latent = latent_tensor[:, :, frame_idx:frame_idx+1, :, :]
                
                # Memory cleanup handled by ComfyUI
                
                # Decode single frame
                single_frame = vae.decode(single_frame_latent)
                all_frames.append(single_frame)
                
                # Cleanup
                del single_frame_latent
                    
            except torch.cuda.OutOfMemoryError:
                print(f"OOM decoding frame {frame_idx + 1}! Trying CPU fallback...")
                try:
                    # Let ComfyUI handle VAE device placement automatically
                    vae_device = vae.device if hasattr(vae, 'device') else 'cuda:0'
                    print(f"Using CPU fallback for frame {frame_idx + 1} - letting ComfyUI handle VAE memory")
                    
                    # Let ComfyUI handle VAE device placement automatically
                    single_frame_latent_cpu = single_frame_latent.cpu()
                    
                    # Decode on CPU (ComfyUI will handle device placement)
                    single_frame_cpu = vae.decode(single_frame_latent_cpu)
                    
                    # Move result back to GPU
                    single_frame = single_frame_cpu.to(vae_device)
                    
                    all_frames.append(single_frame)
                    
                    # Cleanup
                    del single_frame_latent_cpu, single_frame_cpu
                        
                except Exception as cpu_error:
                    print(f"CPU fallback also failed for frame {frame_idx + 1}: {cpu_error}")
                    print(f"Skipping frame {frame_idx + 1}...")
                    # Create dummy frame for this frame
                    dummy_frame = torch.zeros((1, 3, height * 8, width * 8), device=vae_device)
                    all_frames.append(dummy_frame)
                    
                    # Memory cleanup handled by ComfyUI
        
        # Concatenate all frames - ensure all are on the same device
        if all_frames:
            print(f"8a. üîç Concatenating {len(all_frames)} frames...")
            
            # Check device consistency
            devices = [frame.device for frame in all_frames if hasattr(frame, 'device')]
            if devices:
                target_device = devices[0]  # Use first frame's device
                print(f"8a. üîç Target device for concatenation: {target_device}")
                
                # Move all frames to the same device before concatenation
                aligned_frames = []
                for i, frame in enumerate(all_frames):
                    if hasattr(frame, 'device') and frame.device != target_device:
                        print(f"8a. üîß Moving frame {i+1} from {frame.device} to {target_device}")
                        frame = frame.to(target_device)
                    aligned_frames.append(frame)
                
                frames = torch.cat(aligned_frames, dim=0)
                del all_frames, aligned_frames
                print(f"8a. ‚úÖ Concatenation successful on {target_device}")
            else:
                frames = torch.cat(all_frames, dim=0)
                del all_frames
        else:
            # Create empty frames if all failed
            vae_device = vae.device if hasattr(vae, 'device') else 'cuda:0'
            frames = torch.zeros((frames, 3, height * 8, width * 8), device=vae_device)
        
        print(f"Single-frame fallback decoding complete. Output shape: {frames.shape}")
        return frames
    
    def _encode_with_chunking(self, video_generator, positive, negative, vae, width, height, 
                             length, batch_size, strength, control_video, reference_image, 
                             processing_plan, force_downscale=False):
        """Encode video using chunked processing if needed"""
        
        chunk_size = processing_plan['vae_encode']['chunk_size']
        
        if length <= chunk_size:
            # Process all frames at once
            return video_generator.encode(
                positive, negative, vae, width, height, length, batch_size,
                strength, control_video, None, reference_image,
                force_downscale=force_downscale
            )
        
        # Process in chunks
        print(f"Processing {length} frames in chunks of {chunk_size}")
        
        # Use the chunked processor and chunk size
        return video_generator.encode(
            positive, negative, vae, width, height, length, batch_size,
            strength, control_video, None, reference_image,
            chunked_processor=self.chunked_processor,
            chunk_size=chunk_size,
            force_downscale=force_downscale
        )

def main():
    """Main function to run the pipeline"""
    pipeline = ReferenceVideoPipeline()
    
    # Example usage - Updated for individual component loading with absolute paths
    script_dir = Path(__file__).parent
    output_path = pipeline.run_pipeline(
        unet_model_path=str(script_dir / "models/diffusion_models/wan_2.1_diffusion_model.safetensors"),
        clip_model_path=str(script_dir / "models/text_encoders/wan_clip_model.safetensors"),
        vae_model_path=str(script_dir / "models/vaes/wan_vae.safetensors"),
        lora_path=str(script_dir / "models/loras/Wan21_CausVid_14B_T2V_lora_rank32.safetensors"),
        positive_prompt="very cinematic video",
        negative_prompt="Ëâ≤Ë∞ÉËâ≥‰∏ΩÔºåËøáÊõùÔºåÈùôÊÄÅÔºåÁªÜËäÇÊ®°Á≥ä‰∏çÊ∏ÖÔºåÂ≠óÂπïÔºåÈ£éÊ†ºÔºå‰ΩúÂìÅÔºåÁîª‰ΩúÔºåÁîªÈù¢ÔºåÈùôÊ≠¢ÔºåÊï¥‰ΩìÂèëÁÅ∞ÔºåÊúÄÂ∑ÆË¥®ÈáèÔºå‰ΩéË¥®ÈáèÔºåJPEGÂéãÁº©ÊÆãÁïôÔºå‰∏ëÈôãÁöÑÔºåÊÆãÁº∫ÁöÑÔºåÂ§ö‰ΩôÁöÑÊâãÊåáÔºåÁîªÂæó‰∏çÂ•ΩÁöÑÊâãÈÉ®ÔºåÁîªÂæó‰∏çÂ•ΩÁöÑËÑ∏ÈÉ®ÔºåÁï∏ÂΩ¢ÁöÑÔºåÊØÅÂÆπÁöÑÔºåÂΩ¢ÊÄÅÁï∏ÂΩ¢ÁöÑËÇ¢‰ΩìÔºåÊâãÊåáËûçÂêàÔºåÈùôÊ≠¢‰∏çÂä®ÁöÑÁîªÈù¢ÔºåÊùÇ‰π±ÁöÑËÉåÊôØÔºå‰∏âÊù°ËÖøÔºåËÉåÊôØ‰∫∫ÂæàÂ§öÔºåÂÄíÁùÄËµ∞ , extra hands, extra arms, extra legs",
        control_video_path=str(script_dir / "safu.mp4"),
        reference_image_path=str(script_dir / "safu.jpg"),
        width=480,
        height=832,
        length=37,
        output_path="generated_video.mp4"
    )
    
    print(f"Video generated successfully: {output_path}")

if __name__ == "__main__":
    main() 